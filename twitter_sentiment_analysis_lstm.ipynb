{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GM7rGgwF3HP_"
   },
   "source": [
    "# Sentiment Analysis on Twitter tweets using LSTM and Keras\n",
    "<hr>\n",
    "\n",
    "### Steps\n",
    "<ol type=\"1\">\n",
    "    <li>Load the dataset (13k twitter tweets with manually marked label)</li>\n",
    "    <li>Clean Dataset</li>\n",
    "    <li>Encode Sentiments</li>\n",
    "    <li>Split Dataset</li>\n",
    "    <li>Tokenize and Pad/Truncate Tweets</li>\n",
    "    <li>Build Architecture/Model</li>\n",
    "    <li>Train and Test</li>\n",
    "</ol>\n",
    "\n",
    "<hr>\n",
    "<i>Import all the libraries needed</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyZ1H_z93HQE",
    "outputId": "c4f8ab99-2ced-4959-f872-bfab89a9044a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.43.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.23.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Sastrawi\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KXmlGMuU3HQG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re, io, json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory # Indonesian Stemmer\n",
    "import tensorflow as tf \n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3uz-EU93HQH"
   },
   "source": [
    "<hr>\n",
    "<i>Preview dataset</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uoKu5rK3HQI",
    "outputId": "9ea35d7d-6ff3-4f2a-c552-d13ac56f9b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Tweet  HS\n",
      "0      di saat cowok usaha lacak perhati gue kamu lan...   1\n",
      "1      telat beri tau kamu edan sarap gue gaul cigax ...   0\n",
      "2      kadang pikir percaya tuhan jatuh kali kali kad...   0\n",
      "3                                   tau mata sipit lihat   0\n",
      "4        kaum cebong kafir sudah lihat dongok dungu haha   1\n",
      "...                                                  ...  ..\n",
      "13164                   bicara ndasmu congor kate anjing   1\n",
      "13165                                  kasur enak kunyuk   0\n",
      "13166                           hati hati bisu bosan duh   0\n",
      "13167  bom real mudah deteksi bom kubur dahsyat ledak...   0\n",
      "13168                          situ beri foto kutil onta   1\n",
      "\n",
      "[13169 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('clean_dataset.csv')\n",
    "\n",
    "print(data[['Tweet', 'HS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c20YXy9H3HQI"
   },
   "source": [
    "<hr>\n",
    "<b>Stop Word</b> is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n",
    "\n",
    "<i>Declaring the Indonesian stop words</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDupPMYt3HQJ",
    "outputId": "4b0bf9fc-43e6-4afa-d020-e2d86665682a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DISAPP~1\\AppData\\Local\\Temp/ipykernel_12192/3288442261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindonesian_stopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mindonesian_stopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindonesian_stopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mindonesian_stopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "indonesian_stopwords = pd.read_csv('stopwords.txt', sep=\"\\n\")\n",
    "indonesian_stopwords = indonesian_stopwords.iloc[:, 0].values.tolist()\n",
    "indonesian_stopwords.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU91i_l33HQK"
   },
   "source": [
    "Replace alay words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EytaLgdi3HQL"
   },
   "outputs": [],
   "source": [
    "alay_words = pd.read_csv('alay.csv')\n",
    "# alay_words = alay_words.set_index(\"alay\")\n",
    "alay_words\n",
    "\n",
    "row = alay_words[alay_words.alay == \"3x\"]\n",
    "# row.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRa-rrxp_OP-",
    "outputId": "ffa5eda8-cef2-4cfe-8cf9-dbcfae1b032f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiga kali\n"
     ]
    }
   ],
   "source": [
    "# row\n",
    "print(str(row['replacement'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RwLxcC_3HQM"
   },
   "source": [
    "<hr>\n",
    "\n",
    "### Load and Clean Dataset\n",
    "\n",
    "In the original dataset, the tweets are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in <b>load_dataset()</b> function, beside loading the dataset using <b>pandas</b>, I also pre-process the tweets by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the tweets.\n",
    "\n",
    "### Encode Sentiments\n",
    "In the same function, I also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bT1yk-ZR3HQN",
    "outputId": "13ec6372-d9da-4cec-b6da-8c54423fbe93"
   },
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    # Init indonesian stemmer\n",
    "    factory = StemmerFactory()\n",
    "    s = factory.create_stemmer()\n",
    "    result = s.stem(text)\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      if word not in indonesian_stopwords:\n",
    "        output.append(word)\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "def replace_alay(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      row = alay_words[alay_words.alay == word]\n",
    "      if row.empty:\n",
    "        output.append(word)\n",
    "      else:\n",
    "        output.append(str(row['replacement'].values[0]))\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "    \n",
    "\n",
    "def load_dataset():\n",
    "    df = pd.read_csv('utf8_dataset.csv')\n",
    "\n",
    "    # Remove \\n \\t \\r\n",
    "    df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\" \",\" \"], regex=True, inplace=True)\n",
    "\n",
    "    # Tweets/Input\n",
    "    x_data = df['Tweet']\n",
    "    \n",
    "    # Sentiment/Output\n",
    "    y_data = df['HS']\n",
    "\n",
    "    # PRE-PROCESS TWEETS\n",
    "    x_data = x_data.apply(lambda tweet: tweet.lower())\n",
    "\n",
    "    # Remove HTML tags\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)\n",
    "\n",
    "    # Remove non alphabets\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)\n",
    "\n",
    "    # Remove words that is lees than 2 chars\n",
    "    x_data = x_data.apply(lambda tweet: ' '.join([w for w in tweet.split() if len(w) > 2]))\n",
    "\n",
    "    # Remove RT\n",
    "    x_data = x_data.str.replace('rt', '')\n",
    "\n",
    "    # Remove USER\n",
    "    x_data = x_data.str.replace('user', '')\n",
    "\n",
    "    # Remove URL\n",
    "    x_data = x_data.str.replace('url', '')\n",
    "\n",
    "    # Remove excess spaces\n",
    "    x_data = x_data.apply(lambda tweet: ' '.join(tweet.split()))\n",
    "\n",
    "    # Trim\n",
    "    x_data = x_data.str.strip()\n",
    "    \n",
    "    # Remove stop words\n",
    "    x_data = x_data.apply(lambda tweet: remove_stopwords(tweet))\n",
    "\n",
    "    # Replace alay words\n",
    "    x_data = x_data.apply(lambda tweet: replace_alay(tweet))\n",
    "\n",
    "    # Stem\n",
    "    x_data = x_data.apply(lambda tweet: stemmer(tweet))\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace(1, 1)\n",
    "    y_data = y_data.replace(0, 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9Dp1nFHAAIp",
    "outputId": "fdfa7c4b-dfb4-4044-e087-386a576ae264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet\n",
      "0        - disaat semua cowok berusaha melacak perhatia...\n",
      "1        RT USER: USER siapa yang telat ngasih tau elu?...\n",
      "2        41. Kadang aku berfikir, kenapa aku tetap perc...\n",
      "3        USER USER AKU ITU AKU  KU TAU MATAMU SIPIT TAP...\n",
      "4        USER USER Kaum cebong kapir udah keliatan dong...\n",
      "                               ...                        \n",
      "13164    USER jangan asal ngomong ndasmu. congor lu yg ...\n",
      "13165                         USER Kasur mana enak kunyuk'\n",
      "13166    USER Hati hati bisu :( .g  lagi bosan huft \\xf...\n",
      "13167    USER USER USER USER Bom yang real mudah terdet...\n",
      "13168    USER Mana situ ngasih(\": itu cuma foto ya kuti...\n",
      "Name: Tweet, Length: 13169, dtype: object \n",
      "\n",
      "HS\n",
      "0        1\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "13164    1\n",
      "13165    0\n",
      "13166    0\n",
      "13167    0\n",
      "13168    1\n",
      "Name: HS, Length: 13169, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Tweet')\n",
    "print(x_data, '\\n')\n",
    "print('HS')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "XPdha9ZoSDrK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        di saat cowok usaha lacak perhati gue kamu lan...\n",
       "1        telat beri tau kamu edan sarap gue gaul cigax ...\n",
       "2        kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                     tau mata sipit lihat\n",
       "4          kaum cebong kafir sudah lihat dongok dungu haha\n",
       "                               ...                        \n",
       "13164                     bicara ndasmu congor kate anjing\n",
       "13165                                    kasur enak kunyuk\n",
       "13166                             hati hati bisu bosan duh\n",
       "13167    bom real mudah deteksi bom kubur dahsyat ledak...\n",
       "13168                            situ beri foto kutil onta\n",
       "Name: Tweet, Length: 13169, dtype: object"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = x_data.str.replace('uniform resource locator', '')\n",
    "x_data = x_data.apply(lambda tweet: ' '.join([w for w in tweet.split() if not w.startswith('x')]))\n",
    "x_data = x_data.apply(lambda tweet: ' '.join([w for w in tweet.split() if len(w) > 2]))\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.to_csv('clean-tweet.csv')\n",
    "y_data.to_csv('clean-hs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgVJ-zkl3HQN"
   },
   "source": [
    "<hr>\n",
    "\n",
    "### Split Dataset\n",
    "In this work, I decided to split the data into 80% of Training and 20% of Testing set using <b>train_test_split</b> method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the tweets and sentiments are in order, where they list positive tweets first and then negative tweets. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KeCaRd23HQO",
    "outputId": "2c77d348-c343-4d6e-ef93-18ed4fb1cc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "1616                  Tidak melanggar undang-undang apapun\n",
      "11057    1. Siang hari ini, Presiden USER memimpin Rapa...\n",
      "10297    USER Pusing w Uda 2 x Lihat in anak orang eek ...\n",
      "9085     Bukan caci maki lagi, 7 juta bahkan 10 juta si...\n",
      "12051    USER USER USERUSER ABADI AYO DUKUNG TIMNAS IND...\n",
      "                               ...                        \n",
      "4091     USER USER USER USER Dah mas USER gitutu orang ...\n",
      "11459    kedatangan pasangan Hasanah ini untuk meminta ...\n",
      "6885     USER ; Jd kt ente sampa dikolong tol ada, seja...\n",
      "13085    ;Kebanyakan fitnah nih mpok silvy #DebatFinalP...\n",
      "557      Cie tau ma jokowi ga dapet panggung mulai dah ...\n",
      "Name: Tweet, Length: 10535, dtype: object \n",
      "\n",
      "780                                         USER kaya TAI'\n",
      "6172     USER Padahal buaya ngga ada bulu \\xf0\\x9f\\x98\\...\n",
      "2372     USER USER Hati2 antek Yahudi, Liberal, LGBT, S...\n",
      "4492          Pertumbuhan Ekonomi Indonesia Kuartal-I 2018\n",
      "12069                        USER Hahaha payah tai kotok!'\n",
      "                               ...                        \n",
      "1120     USER USER Lp ayahnya kan dah mati bom bunuh di...\n",
      "5711     Menjadi perhatian PENTING dan GENTINg bagi Yg ...\n",
      "7970     USER USER Hmmm kl menurut aku ga dukung sih it...\n",
      "1950     RT USER: Sekali2 yang diangkat \"penyesalan ora...\n",
      "282      USER nek ndelok match terkhir celsi sih kudune...\n",
      "Name: Tweet, Length: 2634, dtype: object \n",
      "\n",
      "Test Set\n",
      "1616     0\n",
      "11057    0\n",
      "10297    0\n",
      "9085     1\n",
      "12051    0\n",
      "        ..\n",
      "4091     1\n",
      "11459    0\n",
      "6885     1\n",
      "13085    1\n",
      "557      1\n",
      "Name: HS, Length: 10535, dtype: int64 \n",
      "\n",
      "780      1\n",
      "6172     0\n",
      "2372     0\n",
      "4492     0\n",
      "12069    1\n",
      "        ..\n",
      "1120     0\n",
      "5711     0\n",
      "7970     0\n",
      "1950     1\n",
      "282      1\n",
      "Name: HS, Length: 2634, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1yPUlEI3HQO"
   },
   "source": [
    "<hr>\n",
    "<i>Function for getting the maximum tweet length, by calculating the mean of all the tweets length (using <b>numpy.mean</b>)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_8RQxb63HQP",
    "outputId": "cbbb47b8-5aed-4508-a2b0-3287c6b6efe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "def get_max_length():\n",
    "    tweet_length = []\n",
    "    for tweet in x_train:\n",
    "        tweet_length.append(len(tweet))\n",
    "\n",
    "    return int(np.ceil(np.mean(tweet_length)))\n",
    "\n",
    "max_length = get_max_length()\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK4CZkth3HQP"
   },
   "source": [
    "<hr>\n",
    "\n",
    "### Tokenize and Pad/Truncate Tweets\n",
    "A Neural Network only accepts numeric data, so we need to encode the tweets. I use <b>tensorflow.keras.preprocessing.text.Tokenizer</b> to encode the tweets into integers, where each unique word is automatically indexed (using <b>fit_on_texts</b> method) based on <b>x_train</b>. <br>\n",
    "<b>x_train</b> and <b>x_test</b> is converted into integers using <b>texts_to_sequences</b> method.\n",
    "\n",
    "Each tweets has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all tweets length) using <b>tensorflow.keras.preprocessing.sequence.pad_sequences</b>.\n",
    "\n",
    "\n",
    "<b>post</b>, pad or truncate the words in the back of a sentence<br>\n",
    "<b>pre</b>, pad or truncate the words in front of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kg9wbw93HQQ",
    "outputId": "3f5b4164-5b7c-40d4-db2f-b88253f03676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[ 269 3212 1734 ...    0    0    0]\n",
      " [  86 6261  120 ...    0    0    0]\n",
      " [   1 6262  756 ...    0    0    0]\n",
      " ...\n",
      " [   1 1699 1780 ...    0    0    0]\n",
      " [4012  539  134 ...    0    0    0]\n",
      " [4871   92  757 ...    0    0    0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[    1   167     0 ...     0     0     0]\n",
      " [    1   739   502 ...     0     0     0]\n",
      " [    1     1  5405 ...     0     0     0]\n",
      " ...\n",
      " [    1     1  5121 ...     0     0     0]\n",
      " [   13     1     8 ...     0     0     0]\n",
      " [    1  2880 18243 ...     0     0     0]] \n",
      "\n",
      "Maximum tweets length:  114\n",
      "Total words:  32659\n"
     ]
    }
   ],
   "source": [
    "# ENCODE TWEETS\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum tweets length: ', max_length)\n",
    "print('Total words: ', total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0xoHCLY3HQQ"
   },
   "source": [
    "<hr>\n",
    "\n",
    "### Build Architecture/Model\n",
    "<b>Embedding Layer</b>: in simple terms, it creates word vectors of each word in the <i>word_index</i> and group words that are related or have similar meaning by analyzing other words around them.\n",
    "\n",
    "<b>LSTM Layer</b>: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n",
    "<ul>\n",
    "    <li><b>Forget Gate</b>, decides information is to be kept or thrown away</li>\n",
    "    <li><b>Input Gate</b>, updates cell state by passing previous output and current input into sigmoid activation function</li>\n",
    "    <li><b>Cell State</b>, calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.</li>\n",
    "    <li><b>Ouput Gate</b>, decides the next hidden state and used for predictions</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dense Layer</b>: compute the input with the weight matrix and bias (optional), and using an activation function. I use <b>Sigmoid</b> activation function for this work because the output is only 0 or 1.\n",
    "\n",
    "The optimizer is <b>Adam</b> and the loss function is <b>Binary Crossentropy</b> because again the output is only 0 and 1, which is a binary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKeWYofv3HQQ",
    "outputId": "5e1c31de-265d-4b56-adc7-1ca24e755b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 114, 32)           1045088   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,069,985\n",
      "Trainable params: 1,069,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# model.add(Embedding(2000, embed_dim,input_length = x_train.shape[1]))\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(2,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeQz0xyb3HQQ"
   },
   "source": [
    "<hr>\n",
    "\n",
    "### Training\n",
    "For training, it is simple. We only need to fit our <b>x_train</b> (input) and <b>y_train</b> (output/label) data. For this training, I use a mini-batch learning method with a <b>batch_size</b> of <i>128</i> and <i>5</i> <b>epochs</b>.\n",
    "\n",
    "Also, I added a callback called **checkpoint** to save the model locally for every epoch if its accuracy improved from the previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0f31GHNq3HQR"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM_2.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffwXZK8w3HQR",
    "outputId": "882fe153-e522-45e6-cfb9-3de4a6d74cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.5743\n",
      "Epoch 00001: accuracy improved from -inf to 0.57428, saving model to models\\LSTM_2.h5\n",
      "83/83 [==============================] - 13s 135ms/step - loss: 0.6824 - accuracy: 0.5743\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.5764\n",
      "Epoch 00002: accuracy improved from 0.57428 to 0.57636, saving model to models\\LSTM_2.h5\n",
      "83/83 [==============================] - 12s 149ms/step - loss: 0.6820 - accuracy: 0.5764\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.5764\n",
      "Epoch 00003: accuracy did not improve from 0.57636\n",
      "83/83 [==============================] - 11s 126ms/step - loss: 0.6818 - accuracy: 0.5764\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6816 - accuracy: 0.5764\n",
      "Epoch 00004: accuracy did not improve from 0.57636\n",
      "83/83 [==============================] - 12s 145ms/step - loss: 0.6816 - accuracy: 0.5764\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.5764\n",
      "Epoch 00005: accuracy did not improve from 0.57636\n",
      "83/83 [==============================] - 13s 160ms/step - loss: 0.6817 - accuracy: 0.5764\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.5764\n",
      "Epoch 00006: accuracy did not improve from 0.57636\n",
      "83/83 [==============================] - 12s 142ms/step - loss: 0.6818 - accuracy: 0.5764\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.5764\n",
      "Epoch 00007: accuracy did not improve from 0.57636\n",
      "83/83 [==============================] - 13s 157ms/step - loss: 0.6824 - accuracy: 0.5764\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.5764\n",
      "Epoch 00008: accuracy did not improve from 0.57636\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 0.6818 - accuracy: 0.5764\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6819 - accuracy: 0.5764\n",
      "Epoch 00009: accuracy did not improve from 0.57636\n",
      "83/83 [==============================] - 12s 147ms/step - loss: 0.6819 - accuracy: 0.5764\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6815 - accuracy: 0.5764\n",
      "Epoch 00010: accuracy did not improve from 0.57636\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 0.6815 - accuracy: 0.5764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c0b0490220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 10, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzD1R4Ba3HQR"
   },
   "source": [
    "<hr>\n",
    "\n",
    "### Testing\n",
    "To evaluate the model, we need to predict the sentiment using our <b>x_test</b> data and comparing the predictions with <b>y_test</b> (expected output) data. Then, we calculate the accuracy of the model by dividing numbers of correct prediction with the total data. Resulted an accuracy of <b>86.63%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aYMweFw3HQS",
    "outputId": "3954716c-db66-4a25-a7a8-e535f21a6966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 1536\n",
      "Wrong Prediction: 1098\n",
      "Accuracy: 58.31435079726651\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict_classes(x_test, batch_size = 128)\n",
    "\n",
    "predict_x = model.predict(x_test, batch_size = 128) \n",
    "y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10492, 44) (10492, 2)\n",
      "(2624, 44) (2624, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 114), found shape=(32, 44)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DISAPP~1\\AppData\\Local\\Temp/ipykernel_29568/3641041911.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mpredict_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mclasses_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 114), found shape=(32, 44)\n"
     ]
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "X = token.texts_to_sequences(data['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(data['HS']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "predict_x = model.predict(X_test) \n",
    "classes_x = np.argmax(predict_x,axis=1)\n",
    "\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':classes_x})\n",
    "print(df_test.head())\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpCRvnxc3HQS"
   },
   "source": [
    "---\n",
    "\n",
    "### Load Saved Model\n",
    "\n",
    "Load saved model and use it to predict a tweet statement's sentiment (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "L4NbEKUn3HQS"
   },
   "outputs": [],
   "source": [
    "loaded_model = load_model('models/LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0TSw3Gv3HQS"
   },
   "source": [
    "Receives a tweet as an input to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "O7l7XTlU3HQS",
    "outputId": "9447fb9a-934d-47f7-b730-b3bd9480e466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: asd\n"
     ]
    }
   ],
   "source": [
    "tweet = str(input('Tweet: '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8rhPjPY3HQT"
   },
   "source": [
    "The input must be pre processed before it is passed to the model to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "k5VMt-HF3HQT",
    "outputId": "98bdc525-654a-4133-ada8-b02dc0b1cd80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  asd\n",
      "Filtered:  ['asd']\n"
     ]
    }
   ],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "tweet = regex.sub('', tweet)\n",
    "print('Cleaned: ', tweet)\n",
    "\n",
    "words = tweet.split(' ')\n",
    "filtered = [w for w in words if w not in indonesian_stopwords]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [tweet.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OjlaChQ3HQT"
   },
   "source": [
    "Once again, we need to tokenize and encode the words. I use the tokenizer which was previously declared because we want to encode the words based on words that are known by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uSPxZ5Ug3HQT",
    "outputId": "628a00c5-3678-4da6-8738-2e01408054cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=get_max_length(), padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3MQ_OV03HQT"
   },
   "source": [
    "This is the result of the prediction which shows the **confidence score** of the tweet statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-lU74xYb3HQT",
    "outputId": "aa34f24a-f808-4937-f85a-d43d9a6f427c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23105481]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZN4yEdR3HQT"
   },
   "source": [
    "If the confidence score is close to 0, then the statement is **negative**. On the other hand, if the confidence score is close to 1, then the statement is **positive**. I use a threshold of **0.7** to determine which confidence score is positive and negative, so if it is equal or greater than 0.7, it is **positive** and if it is less than 0.7, it is **negative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "XZkdZgfE3HQU",
    "outputId": "8662c17a-91d0-4bd4-8483-3c8b72eddc63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0MZxj9b63HQU"
   },
   "outputs": [],
   "source": [
    "tokenizer_json = token.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "twitter-sentiment-analysis-lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
