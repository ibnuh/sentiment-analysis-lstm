{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4",
    "_uuid": "b34dc51c4c60fc1cc8200129e74e7a025fd0cc42"
   },
   "source": [
    "**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n",
    "\n",
    "**What's New** I have added how to deal with data imbalance. Almost all classification task have this problem as number of data of every class if different. For current dataset number of data having positive sentiments is very low relative to data with negative sentiment.\n",
    "\n",
    "**Solving class imbalaned data**:\n",
    "- upsampling \n",
    "- using class weighted loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39c44f0e-d62c-7e11-a542-4fcd58e21442",
    "_uuid": "4efef6a6c3143fbb6bca5903fc1a764bbbb861c4"
   },
   "source": [
    "Using LSTM to classify the movie reviews into positive and negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
    "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re, io, json\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
    "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5"
   },
   "source": [
    "Only keeping the necessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
    "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./clean_dataset.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['Tweet','HS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68989f3bc936825f4425d2d08467ce17c4a2f092"
   },
   "source": [
    "Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f6a102c71c8e281450f7e73a5678cc9d0bb99e99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>di saat cowok usaha lacak perhati gue kamu lan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telat beri tau kamu edan sarap gue gaul cigax ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kadang pikir percaya tuhan jatuh kali kali kad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tau mata sipit lihat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kafir sudah lihat dongok dungu haha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  di saat cowok usaha lacak perhati gue kamu lan...   1\n",
       "1  telat beri tau kamu edan sarap gue gaul cigax ...   0\n",
       "2  kadang pikir percaya tuhan jatuh kali kali kad...   0\n",
       "3                               tau mata sipit lihat   0\n",
       "4    kaum cebong kafir sudah lihat dongok dungu haha   1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
    "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20"
   },
   "source": [
    "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>di saat cowok usaha lacak perhati gue kamu lan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telat beri tau kamu edan sarap gue gaul cigax ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kadang pikir percaya tuhan jatuh kali kali kad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tau mata sipit lihat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kafir sudah lihat dongok dungu haha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bani taplak dan kawan kawan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deklarasi pilih kepala daerah aman anti hoaks ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gue saja selesai watch aldnoah zero kampret me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>admin belanja po baik nak makan ais kepal milo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enak kalau sambil ngewe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  di saat cowok usaha lacak perhati gue kamu lan...   1\n",
       "1  telat beri tau kamu edan sarap gue gaul cigax ...   0\n",
       "2  kadang pikir percaya tuhan jatuh kali kali kad...   0\n",
       "3                               tau mata sipit lihat   0\n",
       "4    kaum cebong kafir sudah lihat dongok dungu haha   1\n",
       "5                        bani taplak dan kawan kawan   1\n",
       "6  deklarasi pilih kepala daerah aman anti hoaks ...   0\n",
       "7  gue saja selesai watch aldnoah zero kampret me...   0\n",
       "8  admin belanja po baik nak makan ais kepal milo...   0\n",
       "9                            enak kalau sambil ngewe   0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# Set all text to be lowercase\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove special chars\n",
    "data['Tweet'] = data['Tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
    "\n",
    "# Define text to remove\n",
    "\n",
    "# data['Tweet'] = data['Tweet'].apply(lambda x: bytes(x, 'utf-8').decode('utf-8', 'ignore'))\n",
    "data['Tweet'] = data['Tweet'].apply((lambda x: ''.join([c for c in x if c not in punctuation])))\n",
    "data['Tweet'] = data['Tweet'].str.replace('rt', '')\n",
    "data['Tweet'] = data['Tweet'].str.replace('user', '')\n",
    "data['Tweet'] = data['Tweet'].str.replace('\\n', '')\n",
    "data['Tweet'] = data['Tweet'].str.strip()\n",
    "\n",
    "data.head(10)\n",
    "# print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13116 Total\n",
      "5553 Hate speech\n",
      "7563 Non hate speech\n"
     ]
    }
   ],
   "source": [
    "print(data['HS'].size, \"Total\")\n",
    "print(np.sum(data['HS'] == 1), \"Hate speech\")\n",
    "print(np.sum(data['HS'] == 0), \"Non hate speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,   57,  336,  165,  598,   10,   23,  598,\n",
       "          10,   81,  139,   23,   23,  336,  211],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0, 1890,  690,\n",
       "          41,   23,  452,  386,   10, 1686,   58],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  599,\n",
       "          94,  149,  198,  506,  129,  129,  599,  198,  216,    5,  482,\n",
       "         982, 1687, 1011,   13,   32,   34, 1786]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(lower=False, num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values)\n",
    "X = tokenizer.texts_to_sequences(data['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('models/LSTM_new_tokenized2.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
    "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89"
   },
   "source": [
    "Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 40, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_3 (Spatia  (None, 40, 128)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
    "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162"
   },
   "source": [
    "Hereby I declare the train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
    "_uuid": "a380bbfae2d098d407b138fc44622c9913a31c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10492, 40) (10492, 2)\n",
      "(2624, 40) (2624, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['HS']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM_new.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
    "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7"
   },
   "source": [
    "Here we train the Network with 15 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.7096\n",
      "Epoch 00001: accuracy improved from -inf to 0.70959, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 28s 315ms/step - loss: 0.5391 - accuracy: 0.7096\n",
      "Epoch 2/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.8432\n",
      "Epoch 00002: accuracy improved from 0.70959 to 0.84321, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 27s 324ms/step - loss: 0.3577 - accuracy: 0.8432\n",
      "Epoch 3/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8616\n",
      "Epoch 00003: accuracy improved from 0.84321 to 0.86161, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 34s 421ms/step - loss: 0.3195 - accuracy: 0.8616\n",
      "Epoch 4/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.8757\n",
      "Epoch 00004: accuracy improved from 0.86161 to 0.87571, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 38s 459ms/step - loss: 0.2944 - accuracy: 0.8757\n",
      "Epoch 5/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.8841\n",
      "Epoch 00005: accuracy improved from 0.87571 to 0.88410, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 38s 469ms/step - loss: 0.2723 - accuracy: 0.8841\n",
      "Epoch 6/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.8913\n",
      "Epoch 00006: accuracy improved from 0.88410 to 0.89125, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 40s 485ms/step - loss: 0.2496 - accuracy: 0.8913\n",
      "Epoch 7/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9041\n",
      "Epoch 00007: accuracy improved from 0.89125 to 0.90412, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 40s 489ms/step - loss: 0.2316 - accuracy: 0.9041\n",
      "Epoch 8/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9103\n",
      "Epoch 00008: accuracy improved from 0.90412 to 0.91031, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 43s 529ms/step - loss: 0.2160 - accuracy: 0.9103\n",
      "Epoch 9/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9198\n",
      "Epoch 00009: accuracy improved from 0.91031 to 0.91984, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 41s 497ms/step - loss: 0.2014 - accuracy: 0.9198\n",
      "Epoch 10/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.9227\n",
      "Epoch 00010: accuracy improved from 0.91984 to 0.92270, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 40s 490ms/step - loss: 0.1867 - accuracy: 0.9227\n",
      "Epoch 11/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9284\n",
      "Epoch 00011: accuracy improved from 0.92270 to 0.92842, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 40s 491ms/step - loss: 0.1742 - accuracy: 0.9284\n",
      "Epoch 12/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.9346\n",
      "Epoch 00012: accuracy improved from 0.92842 to 0.93462, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 41s 494ms/step - loss: 0.1648 - accuracy: 0.9346\n",
      "Epoch 13/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9388\n",
      "Epoch 00013: accuracy improved from 0.93462 to 0.93881, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 41s 496ms/step - loss: 0.1520 - accuracy: 0.9388\n",
      "Epoch 14/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9428\n",
      "Epoch 00014: accuracy improved from 0.93881 to 0.94281, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 46s 563ms/step - loss: 0.1447 - accuracy: 0.9428\n",
      "Epoch 15/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.9441\n",
      "Epoch 00015: accuracy improved from 0.94281 to 0.94405, saving model to models\\LSTM_new.h5\n",
      "82/82 [==============================] - 44s 531ms/step - loss: 0.1376 - accuracy: 0.9441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d7fc60f40>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434",
    "_uuid": "47e99d7ed1f27a85eb01dbafc71b66b329fb1d12"
   },
   "source": [
    "Extracting a validation set, and measuring score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_uuid": "4701961e44bd243e505fc2c1b53b323311ad2b80"
   },
   "outputs": [],
   "source": [
    "# Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "predict_x = model.predict(X_test) \n",
    "classes_x = np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "13d9f618b3b74f881d68aed864a26249d26e63de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     true  pred\n",
      "0  [0, 1]     0\n",
      "1  [1, 0]     0\n",
      "2  [0, 1]     1\n",
      "3  [0, 1]     0\n",
      "4  [1, 0]     0\n",
      "confusion matrix [[1235  272]\n",
      " [ 244  873]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83      1507\n",
      "           1       0.76      0.78      0.77      1117\n",
      "\n",
      "    accuracy                           0.80      2624\n",
      "   macro avg       0.80      0.80      0.80      2624\n",
      "weighted avg       0.80      0.80      0.80      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':classes_x})\n",
    "print(df_test.head())\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf",
    "_uuid": "4b54f18bbf22a953c60f271c318cb076e684df9c"
   },
   "source": [
    "Finally measuring the number of correct guesses.  It is clear that finding negative tweets (**class 0**) goes very well (**recall 0.92**) for the Network but deciding whether is positive (**class 1**) is not really (**recall 0.52**). My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "890a03c9-316e-4d55-98e1-ba29045eff6c",
    "_uuid": "cfcbefe939b72297019e221ca3f5a283974bffff"
   },
   "source": [
    "As expected accuracy for positive data is vary low compare to negative, Lets try to solve this problem.\n",
    "\n",
    "**1. Up-sample Minority Class**\n",
    "\n",
    "Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal.\n",
    "There are several heuristics for doing so, but the most common way is to simply resample with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52"
   },
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "data_majority = data[data['HS'] == 0]\n",
    "data_minority = data[data['HS'] == 1]\n",
    "\n",
    "bias = data_minority.shape[0]/data_majority.shape[0]\n",
    "# lets split train/test data first then \n",
    "train = pd.concat([data_majority.sample(frac=0.8,random_state=200),\n",
    "         data_minority.sample(frac=0.8,random_state=200)])\n",
    "test = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),\n",
    "        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "ce10e582bd894ec271f2587ceb618a9cf8ab03c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive data in training: 4442\n",
      "negative data in training: 6050\n",
      "positive data in test: 1111\n",
      "negative data in test: 1513\n"
     ]
    }
   ],
   "source": [
    "print('positive data in training:',(train.HS == 1).sum())\n",
    "print('negative data in training:',(train.HS == 0).sum())\n",
    "print('positive data in test:',(test.HS == 1).sum())\n",
    "print('negative data in test:',(test.HS == 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority class before upsample: (6050, 2)\n",
      "minority class before upsample: (4442, 2)\n",
      "After upsampling\n",
      "0    6050\n",
      "1    6050\n",
      "Name: HS, dtype: int64\n",
      "x_train shape: (12100, 29)\n",
      "x_test shape (2624, 29)\n"
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes in training data for upsampling \n",
    "data_majority = train[train['HS'] == 0]\n",
    "data_minority = train[train['HS'] == 1]\n",
    "\n",
    "print(\"majority class before upsample:\",data_majority.shape)\n",
    "print(\"minority class before upsample:\",data_minority.shape)\n",
    "\n",
    "# Upsample minority class\n",
    "data_minority_upsampled = resample(data_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "print(\"After upsampling\\n\",data_upsampled.HS.value_counts(),sep = \"\")\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values) # training with whole data\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(data_upsampled['Tweet'].values)\n",
    "X_train = pad_sequences(X_train,maxlen=29)\n",
    "Y_train = pd.get_dummies(data_upsampled['HS']).values\n",
    "print('x_train shape:',X_train.shape)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test['Tweet'].values)\n",
    "X_test = pad_sequences(X_test,maxlen=29)\n",
    "Y_test = pd.get_dummies(test['HS']).values\n",
    "print(\"x_test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 29, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 29, 128)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 192)               246528    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 386       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 502,914\n",
      "Trainable params: 502,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "embed_dim = 128\n",
    "lstm_out = 192\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
    "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7"
   },
   "source": [
    "Here we train the Network. We should run much more than 15 epoch, but I would have to wait forever for kaggle, so it is 15 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "95/95 [==============================] - 16s 141ms/step - loss: 0.7943 - accuracy: 0.6384\n",
      "Epoch 2/15\n",
      "95/95 [==============================] - 14s 151ms/step - loss: 0.4842 - accuracy: 0.8386\n",
      "Epoch 3/15\n",
      "95/95 [==============================] - 18s 194ms/step - loss: 0.4274 - accuracy: 0.8621\n",
      "Epoch 4/15\n",
      "95/95 [==============================] - 17s 175ms/step - loss: 0.3851 - accuracy: 0.8752\n",
      "Epoch 5/15\n",
      "95/95 [==============================] - 17s 176ms/step - loss: 0.3618 - accuracy: 0.8846\n",
      "Epoch 6/15\n",
      "95/95 [==============================] - 19s 200ms/step - loss: 0.3374 - accuracy: 0.8931\n",
      "Epoch 7/15\n",
      "95/95 [==============================] - 17s 178ms/step - loss: 0.3074 - accuracy: 0.9048\n",
      "Epoch 8/15\n",
      "95/95 [==============================] - 17s 174ms/step - loss: 0.2878 - accuracy: 0.9127\n",
      "Epoch 9/15\n",
      "95/95 [==============================] - 20s 213ms/step - loss: 0.2719 - accuracy: 0.9182\n",
      "Epoch 10/15\n",
      "95/95 [==============================] - 18s 190ms/step - loss: 0.2591 - accuracy: 0.9203\n",
      "Epoch 11/15\n",
      "95/95 [==============================] - 18s 192ms/step - loss: 0.2450 - accuracy: 0.9262\n",
      "Epoch 12/15\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.2294 - accuracy: 0.9318\n",
      "Epoch 13/15\n",
      "95/95 [==============================] - 17s 182ms/step - loss: 0.2183 - accuracy: 0.9345\n",
      "Epoch 14/15\n",
      "95/95 [==============================] - 18s 185ms/step - loss: 0.2146 - accuracy: 0.9350\n",
      "Epoch 15/15\n",
      "95/95 [==============================] - 19s 203ms/step - loss: 0.1965 - accuracy: 0.9402s - loss: 0.195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d7e6826d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# also adding weights\n",
    "class_weights = {0: 1 ,\n",
    "                1: 1.6/bias }\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "          class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "2803c5699e0aec22463aadccd1255e63155c1b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1212  301]\n",
      " [ 236  875]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      1513\n",
      "           1       0.74      0.79      0.77      1111\n",
      "\n",
      "    accuracy                           0.80      2624\n",
      "   macro avg       0.79      0.79      0.79      2624\n",
      "weighted avg       0.80      0.80      0.80      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_x = model.predict(X_test) \n",
    "Y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c23e4d6b4cab5b46a58b4c2962abd7a0d29df42"
   },
   "source": [
    "So the class imbalance is reduced significantly recall value for positive tweets (Class 1) improved from 0.54 to 0.77. It is alwayes not possible to reduce it compleatly. \n",
    "\n",
    "You may also noticed that the recall value for Negative tweets also decreased from 0.90 to 0.78  but this can be improved using training model to more epocs and tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "085294a5409b58c2188019177041ceb1756f1826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1212  301]\n",
      " [ 236  875]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      1513\n",
      "           1       0.74      0.79      0.77      1111\n",
      "\n",
      "    accuracy                           0.80      2624\n",
      "   macro avg       0.79      0.79      0.79      2624\n",
      "weighted avg       0.80      0.80      0.80      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running model to few more epochs\n",
    "# model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "#           class_weight=class_weights)\n",
    "\n",
    "predict_x = model.predict(X_test) \n",
    "Y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "# Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
    "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "1/1 - 0s - 32ms/epoch - 32ms/step\n",
      "[0.49630627 0.5036937 ]\n",
      "positive 0.5036937\n",
      "0.5036937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'50.369369984'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt = ['udang']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=40, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "print(sentiment)\n",
    "\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    result = sentiment[0]\n",
    "    print(\"negative\", sentiment[0])\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    result = sentiment[1]\n",
    "    print(\"positive\", sentiment[1])\n",
    "    \n",
    "print(result)\n",
    "format(result * 100, '.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 6]]\n",
      "1/1 - 0s - 229ms/epoch - 229ms/step\n",
      "[0.2712673 0.7287327]\n",
      "1/1 - 0s - 32ms/epoch - 32ms/step\n",
      "[0.80968946 0.19031057]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19031057"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('models/LSTM_new_tokenized2.json') as f:\n",
    "    data = json.load(f)\n",
    "    ttt = tokenizer_from_json(data)\n",
    "\n",
    "loaded_model = load_model('models/LSTM_new.h5')\n",
    "g = ['jokowi']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "g = ttt.texts_to_sequences(g)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "g = pad_sequences(g, maxlen=40, dtype='int32', value=0)\n",
    "print(g)\n",
    "sentiment = loaded_model.predict(g,batch_size=1,verbose = 2)[0]\n",
    "print(sentiment)\n",
    "\n",
    "filtered = ['haduh pergi aja sono']\n",
    "tokenize_words = ttt.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=40, dtype='int32', value=0)\n",
    "sentiment = loaded_model.predict(tokenize_words,batch_size=1,verbose = 2)[0]\n",
    "print(sentiment)\n",
    "np.amin(sentiment)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
