{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce86cd0e",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbed1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Sastrawi --quiet\n",
    "!pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bddc7",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e99eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re, io, json\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Indonesian Stemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d56d7",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "856c9ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>USER Ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deklarasi pilkada 2018 aman dan anti hoax warg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gue baru aja kelar re-watch Aldnoah Zero!!! pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nah admin belanja satu lagi port terbaik nak m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>USER Enak lg klo smbil ngewe'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1\n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0\n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0\n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0\n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1\n",
       "5  USER Ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x...   1\n",
       "6  deklarasi pilkada 2018 aman dan anti hoax warg...   0\n",
       "7  Gue baru aja kelar re-watch Aldnoah Zero!!! pa...   0\n",
       "8  Nah admin belanja satu lagi port terbaik nak m...   0\n",
       "9                      USER Enak lg klo smbil ngewe'   0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./utf8_dataset.csv')\n",
    "data.dropna(subset=['Tweet'], how='all', inplace=True)\n",
    "data = data[['Tweet','HS']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c624c",
   "metadata": {},
   "source": [
    "Check dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "003c7784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13116 Total\n",
      "5553 Hate speech\n",
      "7563 Non hate speech\n"
     ]
    }
   ],
   "source": [
    "print(data['HS'].size, \"Total\")\n",
    "print(np.sum(data['HS'] == 1), \"Hate speech\")\n",
    "print(np.sum(data['HS'] == 0), \"Non hate speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3dbaf",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fa9a8",
   "metadata": {},
   "source": [
    "### Make everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00aed356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    di saat cowok usaha lacak perhati gue kamu lan...\n",
       "1    telat beri tau kamu edan sarap gue gaul cigax ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4      kaum cebong kafir sudah lihat dongok dungu haha\n",
       "5                          bani taplak dan kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue saja selesai watch aldnoah zero kampret me...\n",
       "8    admin belanja po baik nak makan ais kepal milo...\n",
       "9                              enak kalau sambil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: tweet.lower())\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfdea6",
   "metadata": {},
   "source": [
    "### Remove known unwanted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "292c96c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    di saat cowok usaha lacak perhati gue kamu lan...\n",
       "1    telat beri tau kamu edan sarap gue gaul cigax ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4      kaum cebong kafir sudah lihat dongok dungu haha\n",
       "5                          bani taplak dan kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue saja selesai watch aldnoah zero kampret me...\n",
       "8    admin belanja po baik nak makan ais kepal milo...\n",
       "9                              enak kalau sambil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove \\n \\t \\r\n",
    "data['Tweet'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\" \",\" \"], regex=True, inplace=True)\n",
    "\n",
    "# Remove RT\n",
    "data['Tweet'] = data['Tweet'].str.replace('rt', '')\n",
    "\n",
    "# Remove USER\n",
    "data['Tweet'] = data['Tweet'].str.replace('user', '')\n",
    "\n",
    "# Remove URL\n",
    "data['Tweet'] = data['Tweet'].str.replace('url', '')\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c107f",
   "metadata": {},
   "source": [
    "### Remove non-alphabets characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a06da4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    di saat cowok usaha lacak perhati gue kamu lan...\n",
       "1    telat beri tau kamu edan sarap gue gaul cigax ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4      kaum cebong kafir sudah lihat dongok dungu haha\n",
       "5                          bani taplak dan kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue saja selesai watch aldnoah zero kampret me...\n",
       "8    admin belanja po baik nak makan ais kepal milo...\n",
       "9                              enak kalau sambil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].replace({'[^A-Za-z]': ' '}, regex = True)\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f877568",
   "metadata": {},
   "source": [
    "### Remove words that is less than 3 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96aeacc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    saat cowok usaha lacak perhati gue kamu lantas...\n",
      "1    telat beri tau kamu edan sarap gue gaul cigax ...\n",
      "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
      "3                                 tau mata sipit lihat\n",
      "4      kaum cebong kafir sudah lihat dongok dungu haha\n",
      "5                          bani taplak dan kawan kawan\n",
      "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
      "7    gue saja selesai watch aldnoah zero kampret me...\n",
      "8    admin belanja baik nak makan ais kepal milo ai...\n",
      "9                              enak kalau sambil ngewe\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: ' '.join([w for w in tweet.split() if len(w) > 2]))\n",
    "print(data['Tweet'].head(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d94bd",
   "metadata": {},
   "source": [
    "### Reformat texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98a8904e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    saat cowok usaha lacak perhati gue kamu lantas...\n",
       "1    telat beri tau kamu edan sarap gue gaul cigax ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4      kaum cebong kafir sudah lihat dongok dungu haha\n",
       "5                          bani taplak dan kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue saja selesai watch aldnoah zero kampret me...\n",
       "8    admin belanja baik nak makan ais kepal milo ai...\n",
       "9                              enak kalau sambil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove excess spaces\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: ' '.join(tweet.split()))\n",
    "\n",
    "# Trim\n",
    "data['Tweet'] = data['Tweet'].str.strip()\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b1a6a",
   "metadata": {},
   "source": [
    "### Load and replace alay words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc813348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alay</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  alay               replacement\n",
       "0  anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1         pakcikdahtua         pak cik sudah tua\n",
       "2       pakcikmudalagi         pak cik muda lagi\n",
       "3          t3tapjokowi              tetap jokowi\n",
       "4                   3x                 tiga kali\n",
       "5               aamiin                      amin\n",
       "6              aamiinn                      amin\n",
       "7                aamin                      amin\n",
       "8              aammiin                      amin\n",
       "9                 abis                     habis"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alay_words = pd.read_csv('alay.csv')\n",
    "alay_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b12b0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    saat cowok usaha lacak perhati gue kamu lantas...\n",
       "1    telat beri tau kamu edan sarap gue gaul cigax ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4       kaum cebong kafir sudah lihat dungu dungu haha\n",
       "5                          bani taplak dan kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue saja selesai watch aldnoah zero kampret me...\n",
       "8    admin belanja baik nak makan ais kepal milo ai...\n",
       "9                              enak kalau sambil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_alay(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      row = alay_words[alay_words.alay == word]\n",
    "      if row.empty:\n",
    "        output.append(word)\n",
    "      else:\n",
    "        output.append(str(row['replacement'].values[0]))\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: replace_alay(tweet))\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3b404",
   "metadata": {},
   "source": [
    "### Load and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a68eda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adalah',\n",
       " 'adanya',\n",
       " 'adapun',\n",
       " 'agak',\n",
       " 'agaknya',\n",
       " 'agar',\n",
       " 'akan',\n",
       " 'akankah',\n",
       " 'akhir',\n",
       " 'akhiri']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indonesian_stopwords = pd.read_csv('stopwords.txt', sep=\"\\n\")\n",
    "indonesian_stopwords = indonesian_stopwords.iloc[:, 0].values.tolist()\n",
    "indonesian_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2137e0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cowok usaha lacak perhati gue lantas remeh per...\n",
       "1    telat tau edan sarap gue gaul cigax jifla cal ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4             kaum cebong kafir lihat dungu dungu haha\n",
       "5                              bani taplak kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue selesai watch aldnoah zero kampret karakte...\n",
       "8    admin belanja nak makan ais kepal milo ais kep...\n",
       "9                                           enak ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      if word not in indonesian_stopwords:\n",
    "        output.append(word)\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: remove_stopwords(tweet))\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd760794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tau mata sipit lihat'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed10f25",
   "metadata": {},
   "source": [
    "### Stem using Indonesian stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b6f04",
   "metadata": {},
   "source": [
    "It took quite some time, measured to be around 1 hour and 40 minutes, so be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78e28cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: stemmer.stem(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d6fa3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cowok usaha lacak perhati gue lantas remeh per...\n",
       "1    telat tau edan sarap gue gaul cigax jifla cal ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4             kaum cebong kafir lihat dungu dungu haha\n",
       "5                              bani taplak kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue selesai watch aldnoah zero kampret karakte...\n",
       "8    admin belanja nak makan ais kepal milo ais kep...\n",
       "9                                           enak ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235037f1",
   "metadata": {},
   "source": [
    "### Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d7c27c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  283,  132,  518,    7, 1806,  518,\n",
       "           7,   68,  113,  283,  175],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1696,\n",
       "          34,  389,  327,    7, 1495],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,  519,   78,  119,\n",
       "         162,  435,  104,  104,  519,  162,  179,    4,  413,  853, 1496,\n",
       "         880,    9,   27,   29, 1594]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna()\n",
    "\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(lower=False, num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851398f",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Initialize LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3da55002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 38, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 38, 128)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e2c62",
   "metadata": {},
   "source": [
    "### Split dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59de9c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10492, 38) (10492, 2)\n",
      "(2624, 38) (2624, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['HS']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed66cd1",
   "metadata": {},
   "source": [
    "### Declare checkpoint to save the model as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36b9cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/LSTM_twitter_sentiment_analysis.h5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd864094",
   "metadata": {},
   "source": [
    "### Start training with 15 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91f84191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5433 - accuracy: 0.7143\n",
      "Epoch 00001: accuracy improved from -inf to 0.71426, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 25s 275ms/step - loss: 0.5433 - accuracy: 0.7143\n",
      "Epoch 2/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8414\n",
      "Epoch 00002: accuracy improved from 0.71426 to 0.84140, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 22s 265ms/step - loss: 0.3594 - accuracy: 0.8414\n",
      "Epoch 3/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8609\n",
      "Epoch 00003: accuracy improved from 0.84140 to 0.86094, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 22s 273ms/step - loss: 0.3206 - accuracy: 0.8609\n",
      "Epoch 4/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.8702\n",
      "Epoch 00004: accuracy improved from 0.86094 to 0.87019, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 23s 279ms/step - loss: 0.3006 - accuracy: 0.8702\n",
      "Epoch 5/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.8812\n",
      "Epoch 00005: accuracy improved from 0.87019 to 0.88124, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 23s 283ms/step - loss: 0.2797 - accuracy: 0.8812\n",
      "Epoch 6/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.8894\n",
      "Epoch 00006: accuracy improved from 0.88124 to 0.88944, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 25s 305ms/step - loss: 0.2621 - accuracy: 0.8894\n",
      "Epoch 7/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.8979\n",
      "Epoch 00007: accuracy improved from 0.88944 to 0.89792, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 26s 322ms/step - loss: 0.2420 - accuracy: 0.8979\n",
      "Epoch 8/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9028\n",
      "Epoch 00008: accuracy improved from 0.89792 to 0.90278, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 25s 301ms/step - loss: 0.2339 - accuracy: 0.9028\n",
      "Epoch 9/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9123\n",
      "Epoch 00009: accuracy improved from 0.90278 to 0.91231, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 25s 308ms/step - loss: 0.2181 - accuracy: 0.9123\n",
      "Epoch 10/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9141\n",
      "Epoch 00010: accuracy improved from 0.91231 to 0.91413, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 25s 304ms/step - loss: 0.2064 - accuracy: 0.9141\n",
      "Epoch 11/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9202\n",
      "Epoch 00011: accuracy improved from 0.91413 to 0.92022, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 28s 345ms/step - loss: 0.1930 - accuracy: 0.9202\n",
      "Epoch 12/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9247\n",
      "Epoch 00012: accuracy improved from 0.92022 to 0.92470, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 26s 313ms/step - loss: 0.1811 - accuracy: 0.9247\n",
      "Epoch 13/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9290\n",
      "Epoch 00013: accuracy improved from 0.92470 to 0.92899, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 27s 326ms/step - loss: 0.1704 - accuracy: 0.9290\n",
      "Epoch 14/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9335\n",
      "Epoch 00014: accuracy improved from 0.92899 to 0.93347, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 26s 314ms/step - loss: 0.1598 - accuracy: 0.9335\n",
      "Epoch 15/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9355\n",
      "Epoch 00015: accuracy improved from 0.93347 to 0.93547, saving model to models\\LSTM_twitter_sentiment_analysis.h5\n",
      "82/82 [==============================] - 26s 313ms/step - loss: 0.1575 - accuracy: 0.9355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f891683be0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ebc23",
   "metadata": {},
   "source": [
    "### Measure score and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea430414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     true  pred\n",
      "0  [0, 1]     0\n",
      "1  [1, 0]     0\n",
      "2  [0, 1]     1\n",
      "3  [0, 1]     0\n",
      "4  [1, 0]     0\n",
      "confusion matrix [[1229  278]\n",
      " [ 229  888]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83      1507\n",
      "           1       0.76      0.79      0.78      1117\n",
      "\n",
      "    accuracy                           0.81      2624\n",
      "   macro avg       0.80      0.81      0.80      2624\n",
      "weighted avg       0.81      0.81      0.81      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_x = model.predict(X_test)\n",
    "classes_x = np.argmax(predict_x, axis=1)\n",
    "\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred': classes_x})\n",
    "print(df_test.head())\n",
    "\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "print('confusion matrix', confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c2807",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad422381",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02218cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6c90f",
   "metadata": {},
   "source": [
    "### Accept input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd0b6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"jokowi presiden goblog cebong anjing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be44191",
   "metadata": {},
   "source": [
    "### Run preprocessing on the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da4dd925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jokowi presiden goblok cebong anjing'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = replace_alay(tweet)\n",
    "tweet = remove_stopwords(tweet)\n",
    "tweet = stemmer.stem(tweet)\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a1782",
   "metadata": {},
   "source": [
    "### Tokenize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d13fc119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  5  2 69 20 48]]\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = tokenizer.texts_to_sequences([tweet])\n",
    "tokenized_word = pad_sequences(tokenized_word, maxlen=38, dtype='int32', value=0)\n",
    "\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2200076",
   "metadata": {},
   "source": [
    "### Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d84639e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate speech, 0.9927407 sure\n"
     ]
    }
   ],
   "source": [
    "sentiment = loaded_model.predict(tokenized_word,batch_size=1)[0]\n",
    "\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"Not a hate speech,\", sentiment[0], 'sure')\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"Hate speech,\", sentiment[1], 'sure')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
