{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84cd698b",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dceceec",
   "metadata": {},
   "source": [
    "!pip install Sastrawi --quiet\n",
    "!pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f1084",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e2f068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re, io, json\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Indonesian Stemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ecf54",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1484550b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>USER Ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deklarasi pilkada 2018 aman dan anti hoax warg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gue baru aja kelar re-watch Aldnoah Zero!!! pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nah admin belanja satu lagi port terbaik nak m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>USER Enak lg klo smbil ngewe'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1\n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0\n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0\n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0\n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1\n",
       "5  USER Ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x...   1\n",
       "6  deklarasi pilkada 2018 aman dan anti hoax warg...   0\n",
       "7  Gue baru aja kelar re-watch Aldnoah Zero!!! pa...   0\n",
       "8  Nah admin belanja satu lagi port terbaik nak m...   0\n",
       "9                      USER Enak lg klo smbil ngewe'   0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./utf8_dataset.csv')\n",
    "data = data[['Tweet','HS']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0dd8f6",
   "metadata": {},
   "source": [
    "Check dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7289b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13169 Total\n",
      "5561 Hate speech\n",
      "7608 Non hate speech\n"
     ]
    }
   ],
   "source": [
    "print(data['HS'].size, \"Total\")\n",
    "print(np.sum(data['HS'] == 1), \"Hate speech\")\n",
    "print(np.sum(data['HS'] == 0), \"Non hate speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32dc0601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet  HS\n",
      "0  di saat cowok usaha lacak perhati gue kamu lan...   1\n",
      "1  telat beri tau kamu edan sarap gue gaul cigax ...   0\n",
      "2  kadang pikir percaya tuhan jatuh kali kali kad...   0\n",
      "3                               tau mata sipit lihat   0\n",
      "4    kaum cebong kafir sudah lihat dongok dungu haha   1\n",
      "5                        bani taplak dan kawan kawan   1\n",
      "6  deklarasi pilih kepala daerah aman anti hoaks ...   0\n",
      "7  gue saja selesai watch aldnoah zero kampret me...   0\n",
      "8  admin belanja po baik nak makan ais kepal milo...   0\n",
      "9                            enak kalau sambil ngewe   0\n"
     ]
    }
   ],
   "source": [
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "\n",
    "data = pd.read_csv('./clean_dataset.csv')\n",
    "data = data[['Tweet','HS']]\n",
    "print(data.head(10))\n",
    "\n",
    "# Tweets/Input\n",
    "x_data = data['Tweet']\n",
    "\n",
    "# Sentiment/Output\n",
    "y_data = data['HS']\n",
    "\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf82c3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e299b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# Tweets/Input\n",
    "x_data = data['Tweet']\n",
    "\n",
    "# Sentiment/Output\n",
    "y_data = data['HS']\n",
    "\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED\n",
    "# TO BE REMOVED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226de061",
   "metadata": {},
   "source": [
    "### Make everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9957c87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    - disaat semua cowok berusaha melacak perhatia...\n",
       "1    rt user: user siapa yang telat ngasih tau elu?...\n",
       "2    41. kadang aku berfikir, kenapa aku tetap perc...\n",
       "3    user user aku itu aku\\n\\nku tau matamu sipit t...\n",
       "4    user user kaum cebong kapir udah keliatan dong...\n",
       "5    user ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x...\n",
       "6    deklarasi pilkada 2018 aman dan anti hoax warg...\n",
       "7    gue baru aja kelar re-watch aldnoah zero!!! pa...\n",
       "8    nah admin belanja satu lagi port terbaik nak m...\n",
       "9                        user enak lg klo smbil ngewe'\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: tweet.lower())\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c30f71",
   "metadata": {},
   "source": [
    "### Remove known unwanted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76d44969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    - disaat semua cowok berusaha melacak perhatia...\n",
       "1     :  siapa yang telat ngasih tau elu?edan sarap...\n",
       "2    41. kadang aku berfikir, kenapa aku tetap perc...\n",
       "3      aku itu aku  ku tau matamu sipit tapi diliat...\n",
       "4      kaum cebong kapir udah keliatan dongoknya da...\n",
       "5     ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x...\n",
       "6    deklarasi pilkada 2018 aman dan anti hoax warg...\n",
       "7    gue baru aja kelar re-watch aldnoah zero!!! pa...\n",
       "8    nah admin belanja satu lagi po terbaik nak mak...\n",
       "9                             enak lg klo smbil ngewe'\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove \\n \\t \\r\n",
    "data['Tweet'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\" \",\" \"], regex=True, inplace=True)\n",
    "\n",
    "# Remove RT\n",
    "data['Tweet'] = data['Tweet'].str.replace('rt', '')\n",
    "\n",
    "# Remove USER\n",
    "data['Tweet'] = data['Tweet'].str.replace('user', '')\n",
    "\n",
    "# Remove URL\n",
    "data['Tweet'] = data['Tweet'].str.replace('url', '')\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d424a",
   "metadata": {},
   "source": [
    "### Remove non-alphabets characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c93c0c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      disaat semua cowok berusaha melacak perhatia...\n",
       "1        siapa yang telat ngasih tau elu edan sarap...\n",
       "2        kadang aku berfikir  kenapa aku tetap perc...\n",
       "3      aku itu aku  ku tau matamu sipit tapi diliat...\n",
       "4      kaum cebong kapir udah keliatan dongoknya da...\n",
       "5     ya bani taplak dkk  xf  x f x   x   xf  x f x...\n",
       "6    deklarasi pilkada      aman dan anti hoax warg...\n",
       "7    gue baru aja kelar re watch aldnoah zero    pa...\n",
       "8    nah admin belanja satu lagi po terbaik nak mak...\n",
       "9                             enak lg klo smbil ngewe \n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].replace({'[^A-Za-z]': ' '}, regex = True)\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63079542",
   "metadata": {},
   "source": [
    "### Remove words that is less than 3 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "309790d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    disaat semua cowok berusaha melacak perhatian ...\n",
      "1    siapa yang telat ngasih tau elu edan sarap gue...\n",
      "2    kadang aku berfikir kenapa aku tetap percaya p...\n",
      "3    aku itu aku tau matamu sipit tapi diliat dari ...\n",
      "4    kaum cebong kapir udah keliatan dongoknya dari...\n",
      "5                                      bani taplak dkk\n",
      "6    deklarasi pilkada aman dan anti hoax warga duk...\n",
      "7    gue baru aja kelar watch aldnoah zero paling k...\n",
      "8    nah admin belanja satu lagi terbaik nak makan ...\n",
      "9                                 enak klo smbil ngewe\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: ' '.join([w for w in tweet.split() if len(w) > 2]))\n",
    "print(data['Tweet'].head(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f994f28",
   "metadata": {},
   "source": [
    "### Reformat texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13185956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    disaat semua cowok berusaha melacak perhatian ...\n",
       "1    siapa yang telat ngasih tau elu edan sarap gue...\n",
       "2    kadang aku berfikir kenapa aku tetap percaya p...\n",
       "3    aku itu aku tau matamu sipit tapi diliat dari ...\n",
       "4    kaum cebong kapir udah keliatan dongoknya dari...\n",
       "5                                      bani taplak dkk\n",
       "6    deklarasi pilkada aman dan anti hoax warga duk...\n",
       "7    gue baru aja kelar watch aldnoah zero paling k...\n",
       "8    nah admin belanja satu lagi terbaik nak makan ...\n",
       "9                                 enak klo smbil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove excess spaces\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: ' '.join(tweet.split()))\n",
    "\n",
    "# Trim\n",
    "data['Tweet'] = data['Tweet'].str.strip()\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad501e",
   "metadata": {},
   "source": [
    "### Load and replace alay words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07911fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alay</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  alay               replacement\n",
       "0  anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1         pakcikdahtua         pak cik sudah tua\n",
       "2       pakcikmudalagi         pak cik muda lagi\n",
       "3          t3tapjokowi              tetap jokowi\n",
       "4                   3x                 tiga kali\n",
       "5               aamiin                      amin\n",
       "6              aamiinn                      amin\n",
       "7                aamin                      amin\n",
       "8              aammiin                      amin\n",
       "9                 abis                     habis"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alay_words = pd.read_csv('alay.csv')\n",
    "alay_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54e8c4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    di saat semua cowok berusaha melacak perhatian...\n",
       "1    siapa yang telat memberi tau kamu edan sarap g...\n",
       "2    kadang aku berpikir kenapa aku tetap percaya p...\n",
       "3    aku itu aku tau matamu sipit tapi dilihat dari...\n",
       "4    kaum cebong kafir sudah kelihatan dongoknya da...\n",
       "5                          bani taplak dan kawan kawan\n",
       "6    deklarasi pilihan kepala daerah aman dan anti ...\n",
       "7    gue baru saja selesai watch aldnoah zero palin...\n",
       "8    nah admin belanja satu lagi terbaik nak makan ...\n",
       "9                              enak kalau sambil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_alay(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      row = alay_words[alay_words.alay == word]\n",
    "      if row.empty:\n",
    "        output.append(word)\n",
    "      else:\n",
    "        output.append(str(row['replacement'].values[0]))\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: replace_alay(tweet))\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b06f8",
   "metadata": {},
   "source": [
    "### Load and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e70c7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adalah',\n",
       " 'adanya',\n",
       " 'adapun',\n",
       " 'agak',\n",
       " 'agaknya',\n",
       " 'agar',\n",
       " 'akan',\n",
       " 'akankah',\n",
       " 'akhir',\n",
       " 'akhiri']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indonesian_stopwords = pd.read_csv('stopwords.txt', sep=\"\\n\")\n",
    "indonesian_stopwords = indonesian_stopwords.iloc[:, 0].values.tolist()\n",
    "indonesian_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5d679d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cowok berusaha melacak perhatian gue lantas re...\n",
       "1    telat tau edan sarap gue bergaul cigax jifla c...\n",
       "2    kadang berpikir percaya tuhan jatuh berkali ka...\n",
       "3                                     tau matamu sipit\n",
       "4               kaum cebong kafir dongoknya dungu haha\n",
       "5                              bani taplak kawan kawan\n",
       "6    deklarasi pilihan kepala daerah aman anti hoak...\n",
       "7    gue selesai watch aldnoah zero kampret karakte...\n",
       "8    admin belanja terbaik nak makan ais kepal milo...\n",
       "9                                           enak ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      if word not in indonesian_stopwords:\n",
    "        output.append(word)\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: remove_stopwords(tweet))\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d933416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tau matamu sipit'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff602a",
   "metadata": {},
   "source": [
    "### Stem using Indonesian stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fec1e1",
   "metadata": {},
   "source": [
    "It took quite some time, measured to be around 1 hour and 40 minutes, so be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: stemmer.stem(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f7e3181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    di saat cowok usaha lacak perhati gue kamu lan...\n",
       "1    telat beri tau kamu edan sarap gue gaul cigax ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4      kaum cebong kafir sudah lihat dongok dungu haha\n",
       "5                          bani taplak dan kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue saja selesai watch aldnoah zero kampret me...\n",
       "8    admin belanja po baik nak makan ais kepal milo...\n",
       "9                              enak kalau sambil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96ad64",
   "metadata": {},
   "source": [
    "### Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62506939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  253,  693, 1015,    7, 1988, 1015,    7,\n",
       "          73,   97,  253,  144],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1816,\n",
       "          34,  323,  291,    7],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,  436,  852,  159,  133,  668,   94,  436,  133,  892,    4,\n",
       "         184,   23,   32, 1817]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna()\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(lower=False, num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf8ed6",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Initialize LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "360af015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 37, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 37, 128)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 196)               254800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beaa931",
   "metadata": {},
   "source": [
    "### Split dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d3e0bfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10535, 37) (10535, 2)\n",
      "(2634, 37) (2634, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['HS']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f35fc",
   "metadata": {},
   "source": [
    "### Declare checkpoint to save the model as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "25fad5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM_new.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af9ed3",
   "metadata": {},
   "source": [
    "### Start training with 15 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7198d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.7131\n",
      "Epoch 00001: accuracy improved from -inf to 0.71305, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 20s 211ms/step - loss: 0.5350 - accuracy: 0.7131\n",
      "Epoch 2/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3542 - accuracy: 0.8475\n",
      "Epoch 00002: accuracy improved from 0.71305 to 0.84746, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 18s 212ms/step - loss: 0.3542 - accuracy: 0.8475\n",
      "Epoch 3/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3158 - accuracy: 0.8645\n",
      "Epoch 00003: accuracy improved from 0.84746 to 0.86445, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 18s 217ms/step - loss: 0.3158 - accuracy: 0.8645\n",
      "Epoch 4/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.8703\n",
      "Epoch 00004: accuracy improved from 0.86445 to 0.87034, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 19s 226ms/step - loss: 0.3001 - accuracy: 0.8703\n",
      "Epoch 5/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.8812\n",
      "Epoch 00005: accuracy improved from 0.87034 to 0.88116, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 19s 234ms/step - loss: 0.2824 - accuracy: 0.8812\n",
      "Epoch 6/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.8894\n",
      "Epoch 00006: accuracy improved from 0.88116 to 0.88942, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 21s 252ms/step - loss: 0.2644 - accuracy: 0.8894\n",
      "Epoch 7/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.8946\n",
      "Epoch 00007: accuracy improved from 0.88942 to 0.89464, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 22s 263ms/step - loss: 0.2524 - accuracy: 0.8946\n",
      "Epoch 8/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 0.9001\n",
      "Epoch 00008: accuracy improved from 0.89464 to 0.90014, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 21s 255ms/step - loss: 0.2364 - accuracy: 0.9001\n",
      "Epoch 9/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9101\n",
      "Epoch 00009: accuracy improved from 0.90014 to 0.91011, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 22s 268ms/step - loss: 0.2211 - accuracy: 0.9101\n",
      "Epoch 10/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9128\n",
      "Epoch 00010: accuracy improved from 0.91011 to 0.91277, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 23s 273ms/step - loss: 0.2079 - accuracy: 0.9128\n",
      "Epoch 11/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.9168\n",
      "Epoch 00011: accuracy improved from 0.91277 to 0.91675, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 24s 292ms/step - loss: 0.2025 - accuracy: 0.9168\n",
      "Epoch 12/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9223\n",
      "Epoch 00012: accuracy improved from 0.91675 to 0.92226, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 22s 268ms/step - loss: 0.1906 - accuracy: 0.9223\n",
      "Epoch 13/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9257\n",
      "Epoch 00013: accuracy improved from 0.92226 to 0.92568, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 22s 263ms/step - loss: 0.1788 - accuracy: 0.9257\n",
      "Epoch 14/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9277\n",
      "Epoch 00014: accuracy improved from 0.92568 to 0.92767, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 22s 264ms/step - loss: 0.1734 - accuracy: 0.9277\n",
      "Epoch 15/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.9327\n",
      "Epoch 00015: accuracy improved from 0.92767 to 0.93270, saving model to models\\LSTM_new.h5\n",
      "83/83 [==============================] - 23s 274ms/step - loss: 0.1635 - accuracy: 0.9327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a36d9a1be0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa822cb",
   "metadata": {},
   "source": [
    "### Measure score and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1de4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     true  pred\n",
      "0  [0, 1]     1\n",
      "1  [0, 1]     0\n",
      "2  [1, 0]     0\n",
      "3  [1, 0]     0\n",
      "4  [1, 0]     0\n",
      "confusion matrix [[1229  287]\n",
      " [ 218  900]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83      1516\n",
      "           1       0.76      0.81      0.78      1118\n",
      "\n",
      "    accuracy                           0.81      2634\n",
      "   macro avg       0.80      0.81      0.81      2634\n",
      "weighted avg       0.81      0.81      0.81      2634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_x = model.predict(X_test)\n",
    "classes_x = np.argmax(predict_x, axis=1)\n",
    "\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred': classes_x})\n",
    "print(df_test.head())\n",
    "\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "print('confusion matrix', confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d1736",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22a8d7",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1b0d9",
   "metadata": {},
   "source": [
    "### Accept input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43cad3",
   "metadata": {},
   "source": [
    "### Run preprocessing on the input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2535ca",
   "metadata": {},
   "source": [
    "### Tokenize inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c342a7",
   "metadata": {},
   "source": [
    "### Run prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
