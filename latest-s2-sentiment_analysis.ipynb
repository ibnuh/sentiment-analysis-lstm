{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Twitter tweets using LSTM and Keras\n",
    "<hr>\n",
    "\n",
    "### Steps\n",
    "<ol type=\"1\">\n",
    "    <li>Load the dataset (13k twitter tweets with manually marked label)</li>\n",
    "    <li>Clean Dataset</li>\n",
    "    <li>Encode Sentiments</li>\n",
    "    <li>Split Dataset</li>\n",
    "    <li>Tokenize and Pad/Truncate Tweets</li>\n",
    "    <li>Build Architecture/Model</li>\n",
    "    <li>Train and Test</li>\n",
    "</ol>\n",
    "\n",
    "<hr>\n",
    "<i>Import all the libraries needed</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re, io, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<i>Preview dataset</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Tweet  HS\n",
      "0      - disaat semua cowok berusaha melacak perhatia...   1\n",
      "1      RT USER: USER siapa yang telat ngasih tau elu?...   0\n",
      "2      41. Kadang aku berfikir, kenapa aku tetap perc...   0\n",
      "3      USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0\n",
      "4      USER USER Kaum cebong kapir udah keliatan dong...   1\n",
      "...                                                  ...  ..\n",
      "13164  USER jangan asal ngomong ndasmu. congor lu yg ...   1\n",
      "13165                       USER Kasur mana enak kunyuk'   0\n",
      "13166  USER Hati hati bisu :( .g\\n\\nlagi bosan huft \\...   0\n",
      "13167  USER USER USER USER Bom yang real mudah terdet...   0\n",
      "13168  USER Mana situ ngasih(\": itu cuma foto ya kuti...   1\n",
      "\n",
      "[13169 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('utf8_dataset.csv')\n",
    "\n",
    "print(data[['Tweet', 'HS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b>Stop Word</b> is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n",
    "\n",
    "<i>Declaring the Indonesian stop words</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir', 'akhiri', 'akhirnya', 'aku', 'akulah', 'amat', 'amatlah', 'anda', 'andalah', 'antar', 'antara', 'antaranya', 'apa', 'apaan', 'apabila', 'apakah', 'apalagi', 'apatah', 'artinya', 'asal', 'asalkan', 'atas', 'atau', 'ataukah', 'ataupun', 'awal', 'awalnya', 'bagai', 'bagaikan', 'bagaimana', 'bagaimanakah', 'bagaimanapun', 'bagi', 'bagian', 'bahkan', 'bahwa', 'bahwasanya', 'baik', 'bakal', 'bakalan', 'balik', 'banyak', 'bapak', 'baru', 'bawah', 'beberapa', 'begini', 'beginian', 'beginikah', 'beginilah', 'begitu', 'begitukah', 'begitulah', 'begitupun', 'bekerja', 'belakang', 'belakangan', 'belum', 'belumlah', 'benar', 'benarkah', 'benarlah', 'berada', 'berakhir', 'berakhirlah', 'berakhirnya', 'berapa', 'berapakah', 'berapalah', 'berapapun', 'berarti', 'berawal', 'berbagai', 'berdatangan', 'beri', 'berikan', 'berikut', 'berikutnya', 'berjumlah', 'berkali-kali', 'berkata', 'berkehendak', 'berkeinginan', 'berkenaan', 'berlainan', 'berlalu', 'berlangsung', 'berlebihan', 'bermacam', 'bermacam-macam', 'bermaksud', 'bermula', 'bersama', 'bersama-sama', 'bersiap', 'bersiap-siap', 'bertanya', 'bertanya-tanya', 'berturut', 'berturut-turut', 'bertutur', 'berujar', 'berupa', 'besar', 'betul', 'betulkah', 'biasa', 'biasanya', 'bila', 'bilakah', 'bisa', 'bisakah', 'boleh', 'bolehkah', 'bolehlah', 'buat', 'bukan', 'bukankah', 'bukanlah', 'bukannya', 'bulan', 'bung', 'cara', 'caranya', 'cukup', 'cukupkah', 'cukuplah', 'cuma', 'dahulu', 'dalam', 'dan', 'dapat', 'dari', 'daripada', 'datang', 'dekat', 'demi', 'demikian', 'demikianlah', 'dengan', 'depan', 'di', 'dia', 'diakhiri', 'diakhirinya', 'dialah', 'diantara', 'diantaranya', 'diberi', 'diberikan', 'diberikannya', 'dibuat', 'dibuatnya', 'didapat', 'didatangkan', 'digunakan', 'diibaratkan', 'diibaratkannya', 'diingat', 'diingatkan', 'diinginkan', 'dijawab', 'dijelaskan', 'dijelaskannya', 'dikarenakan', 'dikatakan', 'dikatakannya', 'dikerjakan', 'diketahui', 'diketahuinya', 'dikira', 'dilakukan', 'dilalui', 'dilihat', 'dimaksud', 'dimaksudkan', 'dimaksudkannya', 'dimaksudnya', 'diminta', 'dimintai', 'dimisalkan', 'dimulai', 'dimulailah', 'dimulainya', 'dimungkinkan', 'dini', 'dipastikan', 'diperbuat', 'diperbuatnya', 'dipergunakan', 'diperkirakan', 'diperlihatkan', 'diperlukan', 'diperlukannya', 'dipersoalkan', 'dipertanyakan', 'dipunyai', 'diri', 'dirinya', 'disampaikan', 'disebut', 'disebutkan', 'disebutkannya', 'disini', 'disinilah', 'ditambahkan', 'ditandaskan', 'ditanya', 'ditanyai', 'ditanyakan', 'ditegaskan', 'ditujukan', 'ditunjuk', 'ditunjuki', 'ditunjukkan', 'ditunjukkannya', 'ditunjuknya', 'dituturkan', 'dituturkannya', 'diucapkan', 'diucapkannya', 'diungkapkan', 'dong', 'dua', 'dulu', 'empat', 'enggak', 'enggaknya', 'entah', 'entahlah', 'guna', 'gunakan', 'hal', 'hampir', 'hanya', 'hanyalah', 'hari', 'harus', 'haruslah', 'harusnya', 'hendak', 'hendaklah', 'hendaknya', 'hingga', 'ia', 'ialah', 'ibarat', 'ibaratkan', 'ibaratnya', 'ibu', 'ikut', 'ingat', 'ingat-ingat', 'ingin', 'inginkah', 'inginkan', 'ini', 'inikah', 'inilah', 'itu', 'itukah', 'itulah', 'jadi', 'jadilah', 'jadinya', 'jangan', 'jangankan', 'janganlah', 'jauh', 'jawab', 'jawaban', 'jawabnya', 'jelas', 'jelaskan', 'jelaslah', 'jelasnya', 'jika', 'jikalau', 'juga', 'jumlah', 'jumlahnya', 'justru', 'kala', 'kalau', 'kalaulah', 'kalaupun', 'kalian', 'kami', 'kamilah', 'kamu', 'kamulah', 'kan', 'kapan', 'kapankah', 'kapanpun', 'karena', 'karenanya', 'kasus', 'kata', 'katakan', 'katakanlah', 'katanya', 'ke', 'keadaan', 'kebetulan', 'kecil', 'kedua', 'keduanya', 'keinginan', 'kelamaan', 'kelihatan', 'kelihatannya', 'kelima', 'keluar', 'kembali', 'kemudian', 'kemungkinan', 'kemungkinannya', 'kenapa', 'kepada', 'kepadanya', 'kesampaian', 'keseluruhan', 'keseluruhannya', 'keterlaluan', 'ketika', 'khususnya', 'kini', 'kinilah', 'kira', 'kira-kira', 'kiranya', 'kita', 'kitalah', 'kok', 'kurang', 'lagi', 'lagian', 'lah', 'lain', 'lainnya', 'lalu', 'lama', 'lamanya', 'lanjut', 'lanjutnya', 'lebih', 'lewat', 'lima', 'luar', 'macam', 'maka', 'makanya', 'makin', 'malah', 'malahan', 'mampu', 'mampukah', 'mana', 'manakala', 'manalagi', 'masa', 'masalah', 'masalahnya', 'masih', 'masihkah', 'masing', 'masing-masing', 'mau', 'maupun', 'melainkan', 'melakukan', 'melalui', 'melihat', 'melihatnya', 'memang', 'memastikan', 'memberi', 'memberikan', 'membuat', 'memerlukan', 'memihak', 'meminta', 'memintakan', 'memisalkan', 'memperbuat', 'mempergunakan', 'memperkirakan', 'memperlihatkan', 'mempersiapkan', 'mempersoalkan', 'mempertanyakan', 'mempunyai', 'memulai', 'memungkinkan', 'menaiki', 'menambahkan', 'menandaskan', 'menanti', 'menanti-nanti', 'menantikan', 'menanya', 'menanyai', 'menanyakan', 'mendapat', 'mendapatkan', 'mendatang', 'mendatangi', 'mendatangkan', 'menegaskan', 'mengakhiri', 'mengapa', 'mengatakan', 'mengatakannya', 'mengenai', 'mengerjakan', 'mengetahui', 'menggunakan', 'menghendaki', 'mengibaratkan', 'mengibaratkannya', 'mengingat', 'mengingatkan', 'menginginkan', 'mengira', 'mengucapkan', 'mengucapkannya', 'mengungkapkan', 'menjadi', 'menjawab', 'menjelaskan', 'menuju', 'menunjuk', 'menunjuki', 'menunjukkan', 'menunjuknya', 'menurut', 'menuturkan', 'menyampaikan', 'menyangkut', 'menyatakan', 'menyebutkan', 'menyeluruh', 'menyiapkan', 'merasa', 'mereka', 'merekalah', 'merupakan', 'meski', 'meskipun', 'meyakini', 'meyakinkan', 'minta', 'mirip', 'misal', 'misalkan', 'misalnya', 'mula', 'mulai', 'mulailah', 'mulanya', 'mungkin', 'mungkinkah', 'nah', 'naik', 'namun', 'nanti', 'nantinya', 'nyaris', 'nyatanya', 'oleh', 'olehnya', 'pada', 'padahal', 'padanya', 'pak', 'paling', 'panjang', 'pantas', 'para', 'pasti', 'pastilah', 'penting', 'pentingnya', 'per', 'percuma', 'perlu', 'perlukah', 'perlunya', 'pernah', 'persoalan', 'pertama', 'pertama-tama', 'pertanyaan', 'pertanyakan', 'pihak', 'pihaknya', 'pukul', 'pula', 'pun', 'punya', 'rasa', 'rasanya', 'rata', 'rupanya', 'saat', 'saatnya', 'saja', 'sajalah', 'saling', 'sama', 'sama-sama', 'sambil', 'sampai', 'sampai-sampai', 'sampaikan', 'sana', 'sangat', 'sangatlah', 'satu', 'saya', 'sayalah', 'se', 'sebab', 'sebabnya', 'sebagai', 'sebagaimana', 'sebagainya', 'sebagian', 'sebaik', 'sebaik-baiknya', 'sebaiknya', 'sebaliknya', 'sebanyak', 'sebegini', 'sebegitu', 'sebelum', 'sebelumnya', 'sebenarnya', 'seberapa', 'sebesar', 'sebetulnya', 'sebisanya', 'sebuah', 'sebut', 'sebutlah', 'sebutnya', 'secara', 'secukupnya', 'sedang', 'sedangkan', 'sedemikian', 'sedikit', 'sedikitnya', 'seenaknya', 'segala', 'segalanya', 'segera', 'seharusnya', 'sehingga', 'seingat', 'sejak', 'sejauh', 'sejenak', 'sejumlah', 'sekadar', 'sekadarnya', 'sekali', 'sekali-kali', 'sekalian', 'sekaligus', 'sekalipun', 'sekarang', 'sekarang', 'sekecil', 'seketika', 'sekiranya', 'sekitar', 'sekitarnya', 'sekurang-kurangnya', 'sekurangnya', 'sela', 'selain', 'selaku', 'selalu', 'selama', 'selama-lamanya', 'selamanya', 'selanjutnya', 'seluruh', 'seluruhnya', 'semacam', 'semakin', 'semampu', 'semampunya', 'semasa', 'semasih', 'semata', 'semata-mata', 'semaunya', 'sementara', 'semisal', 'semisalnya', 'sempat', 'semua', 'semuanya', 'semula', 'sendiri', 'sendirian', 'sendirinya', 'seolah', 'seolah-olah', 'seorang', 'sepanjang', 'sepantasnya', 'sepantasnyalah', 'seperlunya', 'seperti', 'sepertinya', 'sepihak', 'sering', 'seringnya', 'serta', 'serupa', 'sesaat', 'sesama', 'sesampai', 'sesegera', 'sesekali', 'seseorang', 'sesuatu', 'sesuatunya', 'sesudah', 'sesudahnya', 'setelah', 'setempat', 'setengah', 'seterusnya', 'setiap', 'setiba', 'setibanya', 'setidak-tidaknya', 'setidaknya', 'setinggi', 'seusai', 'sewaktu', 'siap', 'siapa', 'siapakah', 'siapapun', 'sini', 'sinilah', 'soal', 'soalnya', 'suatu', 'sudah', 'sudahkah', 'sudahlah', 'supaya', 'tadi', 'tadinya', 'tahu', 'tahun', 'tak', 'tambah', 'tambahnya', 'tampak', 'tampaknya', 'tandas', 'tandasnya', 'tanpa', 'tanya', 'tanyakan', 'tanyanya', 'tapi', 'tegas', 'tegasnya', 'telah', 'tempat', 'tengah', 'tentang', 'tentu', 'tentulah', 'tentunya', 'tepat', 'terakhir', 'terasa', 'terbanyak', 'terdahulu', 'terdapat', 'terdiri', 'terhadap', 'terhadapnya', 'teringat', 'teringat-ingat', 'terjadi', 'terjadilah', 'terjadinya', 'terkira', 'terlalu', 'terlebih', 'terlihat', 'termasuk', 'ternyata', 'tersampaikan', 'tersebut', 'tersebutlah', 'tertentu', 'tertuju', 'terus', 'terutama', 'tetap', 'tetapi', 'tiap', 'tiba', 'tiba-tiba', 'tidak', 'tidakkah', 'tidaklah', 'tiga', 'tinggi', 'toh', 'tunjuk', 'turut', 'tutur', 'tuturnya', 'ucap', 'ucapnya', 'ujar', 'ujarnya', 'umum', 'umumnya', 'ungkap', 'ungkapnya', 'untuk', 'usah', 'usai', 'waduh', 'wah', 'wahai', 'waktu', 'waktunya', 'walau', 'walaupun', 'wong', 'yaitu', 'yakin', 'yakni', 'yang']\n"
     ]
    }
   ],
   "source": [
    "indonesian_stopwords = pd.read_csv('stopwords.txt', sep=\"\\n\")\n",
    "indonesian_stopwords = indonesian_stopwords.iloc[:, 0].values.tolist()\n",
    "print(indonesian_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Load and Clean Dataset\n",
    "\n",
    "In the original dataset, the tweets are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in <b>load_dataset()</b> function, beside loading the dataset using <b>pandas</b>, I also pre-process the tweets by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the tweets.\n",
    "\n",
    "### Encode Sentiments\n",
    "In the same function, I also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet\n",
      "0        disaat cowok berusaha melacak perhatian gue lo...\n",
      "1        telat ngasih tau elu edan sarap gue bergaul ci...\n",
      "2        kadang berfikir percaya tuhan jatuh berkali ka...\n",
      "3        aku itu aku nku tau matamu sipit tapi diliat d...\n",
      "4        kaum cebong kapir udah keliatan dongoknya dong...\n",
      "                               ...                        \n",
      "13164                  ngomong ndasmu congor sekate anjyng\n",
      "13165                                    kasur enak kunyuk\n",
      "13166                  hati hati bisu nlagi bosan huft xaa\n",
      "13167    bom real mudah terdeteksi bom terkubur dahsyat...\n",
      "13168                     mana situ ngasih foto kutil onta\n",
      "Name: Tweet, Length: 13169, dtype: object \n",
      "\n",
      "HS\n",
      "0        1\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "13164    1\n",
      "13165    0\n",
      "13166    0\n",
      "13167    0\n",
      "13168    1\n",
      "Name: HS, Length: 13169, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('utf8_dataset.csv')\n",
    "    x_data = df['Tweet']       # Tweets/Input\n",
    "    y_data = df['HS']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS TWEETS\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.str.replace('\\n', '')\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda tweet: ' '.join([w for w in tweet.split() if w not in indonesian_stopwords]))  # remove stop words\n",
    "    x_data = x_data.apply(lambda tweet: tweet.lower())   # lower case\n",
    "    x_data = x_data.str.replace('rt', '') # Remove RT\n",
    "    x_data = x_data.str.replace('user', '') # Remove USER\n",
    "    x_data = x_data.str.strip() # Trim\n",
    "    x_data = x_data.apply(lambda tweet: ' '.join(tweet.split()))   # Remove excess spaces\n",
    "    x_data = x_data.apply(lambda tweet: ' '.join([w for w in tweet.split() if len(w) > 2]))  # remove stop words\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace(1, 1)\n",
    "    y_data = y_data.replace(0, 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Tweet')\n",
    "print(x_data, '\\n')\n",
    "print('HS')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Split Dataset\n",
    "In this work, I decided to split the data into 80% of Training and 20% of Testing set using <b>train_test_split</b> method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the tweets and sentiments are in order, where they list positive tweets first and then negative tweets. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "3182           itu disalami jokowi kaya ferdinan antek sby\n",
      "10512    indonesia marah emosi pengen gebukin bilang to...\n",
      "12761    inilah pdkt fisiknya oke kalo diajak ngobrol a...\n",
      "6576                                       sitip nih sitip\n",
      "9773                              emang sipit ceu onih xad\n",
      "                               ...                        \n",
      "3299     mls dduk xleh org budaya dduk xleh budaya heba...\n",
      "515                                          pergi bangkai\n",
      "1411                           berak mencret mencret doang\n",
      "1944     gerindra perusuh prabowo utang gak gak bayar k...\n",
      "6034     diminta sebagai saksi meringankan persidangan ...\n",
      "Name: Tweet, Length: 10535, dtype: object \n",
      "\n",
      "7701                                            congor zon\n",
      "10538          niikmat banget duo cogant abizz ngentot url\n",
      "3368     thn agama buddha sdh kokoh peradaban masyaraka...\n",
      "5526     kalo cewe cowo kapelan manggil daddy baby dite...\n",
      "8040     memang enak komentar situasi sakit hati beli o...\n",
      "                               ...                        \n",
      "155                                 caya mulut buaya bwang\n",
      "2049     kerjasama sisternet kemendes pdtt dorong inter...\n",
      "12725                                mana hadiah nya sipit\n",
      "12698    tidak pecat darmin nasution sbg menko bodoh tu...\n",
      "4307                           ini orang udah tolol nyolot\n",
      "Name: Tweet, Length: 2634, dtype: object \n",
      "\n",
      "Test Set\n",
      "3182     0\n",
      "10512    0\n",
      "12761    0\n",
      "6576     0\n",
      "9773     0\n",
      "        ..\n",
      "3299     0\n",
      "515      1\n",
      "1411     0\n",
      "1944     1\n",
      "6034     0\n",
      "Name: HS, Length: 10535, dtype: int64 \n",
      "\n",
      "7701     1\n",
      "10538    0\n",
      "3368     0\n",
      "5526     1\n",
      "8040     0\n",
      "        ..\n",
      "155      1\n",
      "2049     0\n",
      "12725    0\n",
      "12698    1\n",
      "4307     1\n",
      "Name: HS, Length: 2634, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<i>Function for getting the maximum tweet length, by calculating the mean of all the tweets length (using <b>numpy.mean</b>)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "def get_max_length():\n",
    "    tweet_length = []\n",
    "    for tweet in x_train:\n",
    "        tweet_length.append(len(tweet))\n",
    "\n",
    "    return int(np.ceil(np.mean(tweet_length)))\n",
    "\n",
    "print(get_max_length())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Tokenize and Pad/Truncate Tweets\n",
    "A Neural Network only accepts numeric data, so we need to encode the tweets. I use <b>tensorflow.keras.preprocessing.text.Tokenizer</b> to encode the tweets into integers, where each unique word is automatically indexed (using <b>fit_on_texts</b> method) based on <b>x_train</b>. <br>\n",
    "<b>x_train</b> and <b>x_test</b> is converted into integers using <b>texts_to_sequences</b> method.\n",
    "\n",
    "Each tweets has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all tweets length) using <b>tensorflow.keras.preprocessing.sequence.pad_sequences</b>.\n",
    "\n",
    "\n",
    "<b>post</b>, pad or truncate the words in the back of a sentence<br>\n",
    "<b>pre</b>, pad or truncate the words in front of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[   97  9963     3 ...     0     0     0]\n",
      " [   10   314  1444 ...   594   352   122]\n",
      " [ 1351  9967  6453 ...   327     0     0]\n",
      " ...\n",
      " [ 2378   796   796 ...     0     0     0]\n",
      " [  633 23427    40 ...     0     0     0]\n",
      " [ 3718  1035   896 ...  3832     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[  373  1232     0 ...     0     0     0]\n",
      " [   44 17139   160 ...     0     0     0]\n",
      " [  786     8  1545 ...  2757  1834 14938]\n",
      " ...\n",
      " [  509  2733    11 ...     0     0     0]\n",
      " [  188  1131 19803 ...   201    83   612]\n",
      " [   61     2    22 ...     0     0     0]] \n",
      "\n",
      "Maximum tweets length:  12\n"
     ]
    }
   ],
   "source": [
    "# ENCODE TWEETS\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum tweets length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Build Architecture/Model\n",
    "<b>Embedding Layer</b>: in simple terms, it creates word vectors of each word in the <i>word_index</i> and group words that are related or have similar meaning by analyzing other words around them.\n",
    "\n",
    "<b>LSTM Layer</b>: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n",
    "<ul>\n",
    "    <li><b>Forget Gate</b>, decides information is to be kept or thrown away</li>\n",
    "    <li><b>Input Gate</b>, updates cell state by passing previous output and current input into sigmoid activation function</li>\n",
    "    <li><b>Cell State</b>, calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.</li>\n",
    "    <li><b>Ouput Gate</b>, decides the next hidden state and used for predictions</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dense Layer</b>: compute the input with the weight matrix and bias (optional), and using an activation function. I use <b>Sigmoid</b> activation function for this work because the output is only 0 or 1.\n",
    "\n",
    "The optimizer is <b>Adam</b> and the loss function is <b>Binary Crossentropy</b> because again the output is only 0 and 1, which is a binary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 12, 32)            749728    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 774,625\n",
      "Trainable params: 774,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Training\n",
    "For training, it is simple. We only need to fit our <b>x_train</b> (input) and <b>y_train</b> (output/label) data. For this training, I use a mini-batch learning method with a <b>batch_size</b> of <i>128</i> and <i>5</i> <b>epochs</b>.\n",
    "\n",
    "Also, I added a callback called **checkpoint** to save the model locally for every epoch if its accuracy improved from the previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.6762\n",
      "Epoch 00001: accuracy improved from -inf to 0.67679, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 6s 26ms/step - loss: 0.5690 - accuracy: 0.6768\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8890\n",
      "Epoch 00002: accuracy improved from 0.67679 to 0.88904, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 0.2704 - accuracy: 0.8890\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9480\n",
      "Epoch 00003: accuracy improved from 0.88904 to 0.94798, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 0.1410 - accuracy: 0.9480\n",
      "Epoch 4/10\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9749\n",
      "Epoch 00004: accuracy improved from 0.94798 to 0.97494, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 0.0785 - accuracy: 0.9749\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9856\n",
      "Epoch 00005: accuracy improved from 0.97494 to 0.98557, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 0.0477 - accuracy: 0.9856\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9897\n",
      "Epoch 00006: accuracy improved from 0.98557 to 0.98965, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 0.0350 - accuracy: 0.9897\n",
      "Epoch 7/10\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.0339 - accuracy: 0.9893\n",
      "Epoch 00007: accuracy did not improve from 0.98965\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 0.0335 - accuracy: 0.9895\n",
      "Epoch 8/10\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9927\n",
      "Epoch 00008: accuracy improved from 0.98965 to 0.99279, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 0.0253 - accuracy: 0.9928\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9934\n",
      "Epoch 00009: accuracy improved from 0.99279 to 0.99336, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 2s 25ms/step - loss: 0.0222 - accuracy: 0.9934\n",
      "Epoch 10/10\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9942\n",
      "Epoch 00010: accuracy improved from 0.99336 to 0.99421, saving model to models\\LSTM.h5\n",
      "83/83 [==============================] - 2s 25ms/step - loss: 0.0194 - accuracy: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x253e2988520>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 10, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Testing\n",
    "To evaluate the model, we need to predict the sentiment using our <b>x_test</b> data and comparing the predictions with <b>y_test</b> (expected output) data. Then, we calculate the accuracy of the model by dividing numbers of correct prediction with the total data. Resulted an accuracy of <b>86.63%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 1514\n",
      "Wrong Prediction: 1120\n",
      "Accuracy: 57.4791192103265\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict_classes(x_test, batch_size = 128)\n",
    "\n",
    "predict_x = model.predict(x_test, batch_size = 128) \n",
    "y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load Saved Model\n",
    "\n",
    "Load saved model and use it to predict a tweet statement's sentiment (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('models/LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receives a tweet as an input to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Pendaftaran Capres untuk #Pilpres2019 saja belum dibuka sampai Agustus 2018...yang normatif begini politikus PDIP masih belum paham? #2019GantiPresiden\n"
     ]
    }
   ],
   "source": [
    "tweet = str(input('Tweet: '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input must be pre processed before it is passed to the model to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  Pendaftaran Capres untuk Pilpres saja belum dibuka sampai Agustus yang normatif begini politikus PDIP masih belum paham GantiPresiden\n",
      "Filtered:  ['pendaftaran capres untuk pilpres saja belum dibuka sampai agustus yang normatif begini politikus pdip masih belum paham gantipresiden']\n"
     ]
    }
   ],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "tweet = regex.sub('', tweet)\n",
    "print('Cleaned: ', tweet)\n",
    "\n",
    "words = tweet.split(' ')\n",
    "filtered = [w for w in words if w not in indonesian_stopwords]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [tweet.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to tokenize and encode the words. I use the tokenizer which was previously declared because we want to encode the words based on words that are known by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6444   591   589   319  1764   827  1788  1237  6436    67 22004  2255]]\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=get_max_length(), padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of the prediction which shows the **confidence score** of the tweet statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96300685]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the confidence score is close to 0, then the statement is **negative**. On the other hand, if the confidence score is close to 1, then the statement is **positive**. I use a threshold of **0.7** to determine which confidence score is positive and negative, so if it is equal or greater than 0.7, it is **positive** and if it is less than 0.7, it is **negative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = token.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
