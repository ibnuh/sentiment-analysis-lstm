{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab735f4",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9d6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Sastrawi --quiet\n",
    "!pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855937c3",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faef0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re, io, json\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Indonesian Stemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5256",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30affba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>USER Ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deklarasi pilkada 2018 aman dan anti hoax warg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gue baru aja kelar re-watch Aldnoah Zero!!! pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nah admin belanja satu lagi port terbaik nak m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>USER Enak lg klo smbil ngewe'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1\n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0\n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0\n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0\n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1\n",
       "5  USER Ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x...   1\n",
       "6  deklarasi pilkada 2018 aman dan anti hoax warg...   0\n",
       "7  Gue baru aja kelar re-watch Aldnoah Zero!!! pa...   0\n",
       "8  Nah admin belanja satu lagi port terbaik nak m...   0\n",
       "9                      USER Enak lg klo smbil ngewe'   0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./utf8_dataset.csv')\n",
    "data.dropna(subset=['Tweet'], how='all', inplace=True)\n",
    "data = data[['Tweet','HS']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab48935",
   "metadata": {},
   "source": [
    "Check dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5801e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13169 Total\n",
      "5561 Hate speech\n",
      "7608 Non hate speech\n"
     ]
    }
   ],
   "source": [
    "print(data['HS'].size, \"Total\")\n",
    "print(np.sum(data['HS'] == 1), \"Hate speech\")\n",
    "print(np.sum(data['HS'] == 0), \"Non hate speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712979e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797f442",
   "metadata": {},
   "source": [
    "### Make everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ae18735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    - disaat semua cowok berusaha melacak perhatia...\n",
       "1    rt user: user siapa yang telat ngasih tau elu?...\n",
       "2    41. kadang aku berfikir, kenapa aku tetap perc...\n",
       "3    user user aku itu aku\\n\\nku tau matamu sipit t...\n",
       "4    user user kaum cebong kapir udah keliatan dong...\n",
       "5    user ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x...\n",
       "6    deklarasi pilkada 2018 aman dan anti hoax warg...\n",
       "7    gue baru aja kelar re-watch aldnoah zero!!! pa...\n",
       "8    nah admin belanja satu lagi port terbaik nak m...\n",
       "9                        user enak lg klo smbil ngewe'\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: tweet.lower())\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6f3b5",
   "metadata": {},
   "source": [
    "### Remove known unwanted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29e2e06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    - disaat semua cowok berusaha melacak perhatia...\n",
       "1     :  siapa yang telat ngasih tau elu?edan sarap...\n",
       "2    41. kadang aku berfikir, kenapa aku tetap perc...\n",
       "3      aku itu aku  ku tau matamu sipit tapi diliat...\n",
       "4      kaum cebong kapir udah keliatan dongoknya da...\n",
       "5     ya bani taplak dkk \\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x...\n",
       "6    deklarasi pilkada 2018 aman dan anti hoax warg...\n",
       "7    gue baru aja kelar re-watch aldnoah zero!!! pa...\n",
       "8    nah admin belanja satu lagi po terbaik nak mak...\n",
       "9                             enak lg klo smbil ngewe'\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove \\n \\t \\r\n",
    "data['Tweet'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\" \",\" \"], regex=True, inplace=True)\n",
    "\n",
    "# Remove RT\n",
    "data['Tweet'] = data['Tweet'].str.replace('rt', '')\n",
    "\n",
    "# Remove USER\n",
    "data['Tweet'] = data['Tweet'].str.replace('user', '')\n",
    "\n",
    "# Remove URL\n",
    "data['Tweet'] = data['Tweet'].str.replace('url', '')\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58a872",
   "metadata": {},
   "source": [
    "### Remove non-alphabets characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0164bfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      disaat semua cowok berusaha melacak perhatia...\n",
       "1        siapa yang telat ngasih tau elu edan sarap...\n",
       "2        kadang aku berfikir  kenapa aku tetap perc...\n",
       "3      aku itu aku  ku tau matamu sipit tapi diliat...\n",
       "4      kaum cebong kapir udah keliatan dongoknya da...\n",
       "5     ya bani taplak dkk  xf  x f x   x   xf  x f x...\n",
       "6    deklarasi pilkada      aman dan anti hoax warg...\n",
       "7    gue baru aja kelar re watch aldnoah zero    pa...\n",
       "8    nah admin belanja satu lagi po terbaik nak mak...\n",
       "9                             enak lg klo smbil ngewe \n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].replace({'[^A-Za-z]': ' '}, regex = True)\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fb0e7",
   "metadata": {},
   "source": [
    "### Remove words that is less than 3 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "163e58bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    disaat semua cowok berusaha melacak perhatian ...\n",
      "1    siapa yang telat ngasih tau elu edan sarap gue...\n",
      "2    kadang aku berfikir kenapa aku tetap percaya p...\n",
      "3    aku itu aku tau matamu sipit tapi diliat dari ...\n",
      "4    kaum cebong kapir udah keliatan dongoknya dari...\n",
      "5                                      bani taplak dkk\n",
      "6    deklarasi pilkada aman dan anti hoax warga duk...\n",
      "7    gue baru aja kelar watch aldnoah zero paling k...\n",
      "8    nah admin belanja satu lagi terbaik nak makan ...\n",
      "9                                 enak klo smbil ngewe\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: ' '.join([w for w in tweet.split() if len(w) > 2]))\n",
    "print(data['Tweet'].head(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf0fc4",
   "metadata": {},
   "source": [
    "### Reformat texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ab714e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    disaat semua cowok berusaha melacak perhatian ...\n",
       "1    siapa yang telat ngasih tau elu edan sarap gue...\n",
       "2    kadang aku berfikir kenapa aku tetap percaya p...\n",
       "3    aku itu aku tau matamu sipit tapi diliat dari ...\n",
       "4    kaum cebong kapir udah keliatan dongoknya dari...\n",
       "5                                      bani taplak dkk\n",
       "6    deklarasi pilkada aman dan anti hoax warga duk...\n",
       "7    gue baru aja kelar watch aldnoah zero paling k...\n",
       "8    nah admin belanja satu lagi terbaik nak makan ...\n",
       "9                                 enak klo smbil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove excess spaces\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: ' '.join(tweet.split()))\n",
    "\n",
    "# Trim\n",
    "data['Tweet'] = data['Tweet'].str.strip()\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569dd46",
   "metadata": {},
   "source": [
    "### Load and replace alay words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f59e29fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alay</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  alay               replacement\n",
       "0  anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1         pakcikdahtua         pak cik sudah tua\n",
       "2       pakcikmudalagi         pak cik muda lagi\n",
       "3          t3tapjokowi              tetap jokowi\n",
       "4                   3x                 tiga kali\n",
       "5               aamiin                      amin\n",
       "6              aamiinn                      amin\n",
       "7                aamin                      amin\n",
       "8              aammiin                      amin\n",
       "9                 abis                     habis"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alay_words = pd.read_csv('alay.csv')\n",
    "alay_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "069429f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    di saat semua cowok berusaha melacak perhatian...\n",
       "1    siapa yang telat memberi tau kamu edan sarap g...\n",
       "2    kadang aku berpikir kenapa aku tetap percaya p...\n",
       "3    aku itu aku tau matamu sipit tapi dilihat dari...\n",
       "4    kaum cebong kafir sudah kelihatan dongoknya da...\n",
       "5                          bani taplak dan kawan kawan\n",
       "6    deklarasi pilihan kepala daerah aman dan anti ...\n",
       "7    gue baru saja selesai watch aldnoah zero palin...\n",
       "8    nah admin belanja satu lagi terbaik nak makan ...\n",
       "9                              enak kalau sambil ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_alay(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      row = alay_words[alay_words.alay == word]\n",
    "      if row.empty:\n",
    "        output.append(word)\n",
    "      else:\n",
    "        output.append(str(row['replacement'].values[0]))\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: replace_alay(tweet))\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cb7f5",
   "metadata": {},
   "source": [
    "### Load and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56df2671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adalah',\n",
       " 'adanya',\n",
       " 'adapun',\n",
       " 'agak',\n",
       " 'agaknya',\n",
       " 'agar',\n",
       " 'akan',\n",
       " 'akankah',\n",
       " 'akhir',\n",
       " 'akhiri']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indonesian_stopwords = pd.read_csv('stopwords.txt', sep=\"\\n\")\n",
    "indonesian_stopwords = indonesian_stopwords.iloc[:, 0].values.tolist()\n",
    "indonesian_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61b09821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cowok berusaha melacak perhatian gue lantas re...\n",
       "1    telat tau edan sarap gue bergaul cigax jifla c...\n",
       "2    kadang berpikir percaya tuhan jatuh berkali ka...\n",
       "3                                     tau matamu sipit\n",
       "4               kaum cebong kafir dongoknya dungu haha\n",
       "5                              bani taplak kawan kawan\n",
       "6    deklarasi pilihan kepala daerah aman anti hoak...\n",
       "7    gue selesai watch aldnoah zero kampret karakte...\n",
       "8    admin belanja terbaik nak makan ais kepal milo...\n",
       "9                                           enak ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      if word not in indonesian_stopwords:\n",
    "        output.append(word)\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: remove_stopwords(tweet))\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97bc9be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tau matamu sipit'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9caf38d",
   "metadata": {},
   "source": [
    "### Stem using Indonesian stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419362d8",
   "metadata": {},
   "source": [
    "It took quite some time, measured to be around 1 hour and 40 minutes, so be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60bff5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: stemmer.stem(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4936af4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cowok usaha lacak perhati gue lantas remeh per...\n",
       "1    telat tau edan sarap gue gaul cigax jifla cal ...\n",
       "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
       "3                                 tau mata sipit lihat\n",
       "4            kaum cebong kafir lihat dongok dungu haha\n",
       "5                              bani taplak kawan kawan\n",
       "6    deklarasi pilih kepala daerah aman anti hoaks ...\n",
       "7    gue selesai watch aldnoah zero kampret karakte...\n",
       "8    admin belanja po nak makan ais kepal milo ais ...\n",
       "9                                           enak ngewe\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc92113",
   "metadata": {},
   "source": [
    "### Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73d0505d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  286,  132,  523,    7, 1824,  523,\n",
       "           7,   67,  114,  286,  176],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1715,\n",
       "          34,  391,  329,    7, 1515],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,  524,   76,  119,\n",
       "         163,  440,  104,  104,  524,  163,  180,    4,  417,  862, 1516,\n",
       "         889,    9,   27,   29, 1613]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna()\n",
    "\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(lower=False, num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742450f",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Initialize LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0141e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 38, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 38, 128)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 196)               254800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abf457",
   "metadata": {},
   "source": [
    "### Split dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61f110f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10492, 38) (10492, 2)\n",
      "(2624, 38) (2624, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['HS']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d793971",
   "metadata": {},
   "source": [
    "### Declare checkpoint to save the model as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbb73c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/LSTM_twitter_sentiment_analysis_latest.h5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df2a6e",
   "metadata": {},
   "source": [
    "### Start training with 15 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2315370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5492 - accuracy: 0.7074\n",
      "Epoch 00001: accuracy improved from -inf to 0.70740, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 18s 191ms/step - loss: 0.5492 - accuracy: 0.7074\n",
      "Epoch 2/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3597 - accuracy: 0.8390\n",
      "Epoch 00002: accuracy improved from 0.70740 to 0.83902, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 18s 226ms/step - loss: 0.3597 - accuracy: 0.8390\n",
      "Epoch 3/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.8613\n",
      "Epoch 00003: accuracy improved from 0.83902 to 0.86132, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 20s 248ms/step - loss: 0.3184 - accuracy: 0.8613\n",
      "Epoch 4/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8694\n",
      "Epoch 00004: accuracy improved from 0.86132 to 0.86942, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 20s 249ms/step - loss: 0.3024 - accuracy: 0.8694\n",
      "Epoch 5/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.8766\n",
      "Epoch 00005: accuracy improved from 0.86942 to 0.87657, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 20s 242ms/step - loss: 0.2882 - accuracy: 0.8766\n",
      "Epoch 6/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.8878\n",
      "Epoch 00006: accuracy improved from 0.87657 to 0.88782, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 21s 255ms/step - loss: 0.2671 - accuracy: 0.8878\n",
      "Epoch 7/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.8959\n",
      "Epoch 00007: accuracy improved from 0.88782 to 0.89592, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 26s 312ms/step - loss: 0.2497 - accuracy: 0.8959\n",
      "Epoch 8/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9031\n",
      "Epoch 00008: accuracy improved from 0.89592 to 0.90307, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.2355 - accuracy: 0.9031\n",
      "Epoch 9/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9067\n",
      "Epoch 00009: accuracy improved from 0.90307 to 0.90669, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 22s 265ms/step - loss: 0.2220 - accuracy: 0.9067\n",
      "Epoch 10/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9124\n",
      "Epoch 00010: accuracy improved from 0.90669 to 0.91241, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.2146 - accuracy: 0.9124\n",
      "Epoch 11/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.9160\n",
      "Epoch 00011: accuracy improved from 0.91241 to 0.91603, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.2011 - accuracy: 0.9160\n",
      "Epoch 12/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9228\n",
      "Epoch 00012: accuracy improved from 0.91603 to 0.92280, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 23s 283ms/step - loss: 0.1876 - accuracy: 0.9228\n",
      "Epoch 13/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9250\n",
      "Epoch 00013: accuracy improved from 0.92280 to 0.92499, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.1816 - accuracy: 0.9250\n",
      "Epoch 14/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9298\n",
      "Epoch 00014: accuracy improved from 0.92499 to 0.92976, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 26s 314ms/step - loss: 0.1716 - accuracy: 0.9298\n",
      "Epoch 15/15\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9356\n",
      "Epoch 00015: accuracy improved from 0.92976 to 0.93557, saving model to models\\LSTM_twitter_sentiment_analysis_latest.h5\n",
      "82/82 [==============================] - 26s 313ms/step - loss: 0.1622 - accuracy: 0.9356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15d5cd1eb80>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e00602",
   "metadata": {},
   "source": [
    "### Measure score and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b95b631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     true  pred\n",
      "0  [0, 1]     0\n",
      "1  [1, 0]     0\n",
      "2  [0, 1]     1\n",
      "3  [0, 1]     0\n",
      "4  [1, 0]     0\n",
      "confusion matrix [[1228  279]\n",
      " [ 222  895]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83      1507\n",
      "           1       0.76      0.80      0.78      1117\n",
      "\n",
      "    accuracy                           0.81      2624\n",
      "   macro avg       0.80      0.81      0.81      2624\n",
      "weighted avg       0.81      0.81      0.81      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_x = model.predict(X_test)\n",
    "classes_x = np.argmax(predict_x, axis=1)\n",
    "\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred': classes_x})\n",
    "print(df_test.head())\n",
    "\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "print('confusion matrix', confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8d48b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b46cd",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1ca33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a329c",
   "metadata": {},
   "source": [
    "### Accept input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6f13cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"itu cebong ngapain demo di monas, mending tiduran dirumah\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7c540",
   "metadata": {},
   "source": [
    "### Run preprocessing on the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95497671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cebong demo monas mending tidur rumah'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = replace_alay(tweet)\n",
    "tweet = remove_stopwords(tweet)\n",
    "tweet = stemmer.stem(tweet)\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484a81f",
   "metadata": {},
   "source": [
    "### Tokenize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17384ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20 278 459\n",
      "  422 150]]\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = tokenizer.texts_to_sequences([tweet])\n",
    "tokenized_word = pad_sequences(tokenized_word, maxlen=38, dtype='int32', value=0)\n",
    "\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76c3ba",
   "metadata": {},
   "source": [
    "### Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69717cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate speech, 0.97856903 sure\n"
     ]
    }
   ],
   "source": [
    "sentiment = loaded_model.predict(tokenized_word,batch_size=1)[0]\n",
    "\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"Not a hate speech,\", sentiment[0], 'sure')\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"Hate speech,\", sentiment[1], 'sure')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
