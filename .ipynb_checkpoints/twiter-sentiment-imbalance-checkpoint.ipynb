{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4",
    "_uuid": "b34dc51c4c60fc1cc8200129e74e7a025fd0cc42"
   },
   "source": [
    "**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n",
    "\n",
    "**What's New** I have added how to deal with data imbalance. Almost all classification task have this problem as number of data of every class if different. For current dataset number of data having positive sentiments is very low relative to data with negative sentiment.\n",
    "\n",
    "**Solving class imbalaned data**:\n",
    "- upsampling \n",
    "- using class weighted loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39c44f0e-d62c-7e11-a542-4fcd58e21442",
    "_uuid": "4efef6a6c3143fbb6bca5903fc1a764bbbb861c4"
   },
   "source": [
    "Using LSTM to classify the movie reviews into positive and negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
    "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
    "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5"
   },
   "source": [
    "Only keeping the necessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
    "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./utf8_dataset.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['Tweet','HS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68989f3bc936825f4425d2d08467ce17c4a2f092"
   },
   "source": [
    "Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f6a102c71c8e281450f7e73a5678cc9d0bb99e99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1\n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0\n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0\n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0\n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
    "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20"
   },
   "source": [
    "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disaat semua cowok berusaha melacak perhatian ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siapa yang telat ngasih tau eluedan sarap gue ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41 kadang aku berfikir kenapa aku tetap percay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aku itu akunnku tau matamu sipit tapi diliat d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kapir udah keliatan dongoknya dari...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ya bani taplak dkk xf0x9fx98x84xf0x9fx98x84xf0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deklarasi pilkada 2018 aman dan anti hoax warg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gue baru aja kelar rewatch aldnoah zero paling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nah admin belanja satu lagi po terbaik nak mak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enak lg klo smbil ngewe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  disaat semua cowok berusaha melacak perhatian ...   1\n",
       "1  siapa yang telat ngasih tau eluedan sarap gue ...   0\n",
       "2  41 kadang aku berfikir kenapa aku tetap percay...   0\n",
       "3  aku itu akunnku tau matamu sipit tapi diliat d...   0\n",
       "4  kaum cebong kapir udah keliatan dongoknya dari...   1\n",
       "5  ya bani taplak dkk xf0x9fx98x84xf0x9fx98x84xf0...   1\n",
       "6  deklarasi pilkada 2018 aman dan anti hoax warg...   0\n",
       "7  gue baru aja kelar rewatch aldnoah zero paling...   0\n",
       "8  nah admin belanja satu lagi po terbaik nak mak...   0\n",
       "9                            enak lg klo smbil ngewe   0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# Set all text to be lowercase\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove special chars\n",
    "data['Tweet'] = data['Tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
    "\n",
    "# Define text to remove\n",
    "\n",
    "# data['Tweet'] = data['Tweet'].apply(lambda x: bytes(x, 'utf-8').decode('utf-8', 'ignore'))\n",
    "data['Tweet'] = data['Tweet'].apply((lambda x: ''.join([c for c in x if c not in punctuation])))\n",
    "data['Tweet'] = data['Tweet'].str.replace('rt', '')\n",
    "data['Tweet'] = data['Tweet'].str.replace('user', '')\n",
    "data['Tweet'] = data['Tweet'].str.replace('\\n', '')\n",
    "data['Tweet'] = data['Tweet'].str.strip()\n",
    "\n",
    "data.head(10)\n",
    "# print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11122 Hate speech\n",
      "15216 Non hate speech\n"
     ]
    }
   ],
   "source": [
    "print(data[ data['HS'] == 1].size, \"Hate speech\")\n",
    "print(data[ data['HS'] == 0].size, \"Non hate speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,   57,  989,  926, 1467,\n",
       "          43,  315, 1467,    1,   43,  217,  683,   25,  927,  927,  989,\n",
       "         268],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  126,    4, 1356,   96,  493,   43,   27,   20,  126,\n",
       "          24],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,  790,   22,  158,   22,\n",
       "         244,  319,   49,  252,  284,   22,  173,  928,  790,   22,  437,\n",
       "         252,    5,   22,  338,  338,   53,  567,   11,   70,  338,   22,\n",
       "         101]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values)\n",
    "X = tokenizer.texts_to_sequences(data['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
    "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89"
   },
   "source": [
    "Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 45, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 45, 128)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 196)               254800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
    "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162"
   },
   "source": [
    "Hereby I declare the train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
    "_uuid": "a380bbfae2d098d407b138fc44622c9913a31c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10535, 45) (10535, 2)\n",
      "(2634, 45) (2634, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['HS']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
    "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7"
   },
   "source": [
    "Here we train the Network with 15 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "83/83 [==============================] - 26s 283ms/step - loss: 0.5389 - accuracy: 0.7080\n",
      "Epoch 2/15\n",
      "38/83 [============>.................] - ETA: 14s - loss: 0.3418 - accuracy: 0.8546"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434",
    "_uuid": "47e99d7ed1f27a85eb01dbafc71b66b329fb1d12"
   },
   "source": [
    "Extracting a validation set, and measuring score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "_uuid": "4701961e44bd243e505fc2c1b53b323311ad2b80"
   },
   "outputs": [],
   "source": [
    "# Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "predict_x = model.predict(X_test) \n",
    "classes_x = np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "_uuid": "13d9f618b3b74f881d68aed864a26249d26e63de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1661  973]\n",
      " [   0    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.63      0.77      2634\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.63      2634\n",
      "   macro avg       0.50      0.32      0.39      2634\n",
      "weighted avg       1.00      0.63      0.77      2634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame({'true': classes_x.tolist(), 'pred':classes_x})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf",
    "_uuid": "4b54f18bbf22a953c60f271c318cb076e684df9c"
   },
   "source": [
    "Finally measuring the number of correct guesses.  It is clear that finding negative tweets (**class 0**) goes very well (**recall 0.92**) for the Network but deciding whether is positive (**class 1**) is not really (**recall 0.52**). My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "890a03c9-316e-4d55-98e1-ba29045eff6c",
    "_uuid": "cfcbefe939b72297019e221ca3f5a283974bffff"
   },
   "source": [
    "As expected accuracy for positive data is vary low compare to negative, Lets try to solve this problem.\n",
    "\n",
    "**1. Up-sample Minority Class**\n",
    "\n",
    "Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal.\n",
    "There are several heuristics for doing so, but the most common way is to simply resample with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52"
   },
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "data_majority = data[data['Abusive'] == 0]\n",
    "data_minority = data[data['Abusive'] == 1]\n",
    "\n",
    "bias = data_minority.shape[0]/data_majority.shape[0]\n",
    "# lets split train/test data first then \n",
    "train = pd.concat([data_majority.sample(frac=0.8,random_state=200),\n",
    "         data_minority.sample(frac=0.8,random_state=200)])\n",
    "test = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),\n",
    "        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "_uuid": "ce10e582bd894ec271f2587ceb618a9cf8ab03c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive data in training: 4034\n",
      "negative data in training: 6501\n",
      "positive data in test: 1009\n",
      "negative data in test: 1625\n"
     ]
    }
   ],
   "source": [
    "print('positive data in training:',(train.Abusive == 1).sum())\n",
    "print('negative data in training:',(train.Abusive == 0).sum())\n",
    "print('positive data in test:',(test.Abusive == 1).sum())\n",
    "print('negative data in test:',(test.Abusive == 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority class before upsample: (6501, 2)\n",
      "minority class before upsample: (4034, 2)\n",
      "After upsampling\n",
      "0    6501\n",
      "1    6501\n",
      "Name: Abusive, dtype: int64\n",
      "x_train shape: (13002, 29)\n",
      "x_test shape (2634, 29)\n"
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes in training data for upsampling \n",
    "data_majority = train[train['Abusive'] == 0]\n",
    "data_minority = train[train['Abusive'] == 1]\n",
    "\n",
    "print(\"majority class before upsample:\",data_majority.shape)\n",
    "print(\"minority class before upsample:\",data_minority.shape)\n",
    "\n",
    "# Upsample minority class\n",
    "data_minority_upsampled = resample(data_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "print(\"After upsampling\\n\",data_upsampled.Abusive.value_counts(),sep = \"\")\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values) # training with whole data\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(data_upsampled['Tweet'].values)\n",
    "X_train = pad_sequences(X_train,maxlen=29)\n",
    "Y_train = pd.get_dummies(data_upsampled['Abusive']).values\n",
    "print('x_train shape:',X_train.shape)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test['Tweet'].values)\n",
    "X_test = pad_sequences(X_test,maxlen=29)\n",
    "Y_test = pd.get_dummies(test['Abusive']).values\n",
    "print(\"x_test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 29, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spatia  (None, 29, 128)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 192)               246528    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 386       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 502,914\n",
      "Trainable params: 502,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "embed_dim = 128\n",
    "lstm_out = 192\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
    "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7"
   },
   "source": [
    "Here we train the Network. We should run much more than 15 epoch, but I would have to wait forever for kaggle, so it is 15 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "102/102 [==============================] - 23s 200ms/step - loss: 0.7649 - accuracy: 0.6831\n",
      "Epoch 2/15\n",
      "102/102 [==============================] - 21s 202ms/step - loss: 0.3687 - accuracy: 0.8951\n",
      "Epoch 3/15\n",
      "102/102 [==============================] - 21s 211ms/step - loss: 0.2776 - accuracy: 0.9226\n",
      "Epoch 4/15\n",
      "102/102 [==============================] - 24s 232ms/step - loss: 0.2425 - accuracy: 0.9331\n",
      "Epoch 5/15\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 0.2206 - accuracy: 0.9411\n",
      "Epoch 6/15\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 0.2112 - accuracy: 0.9433\n",
      "Epoch 7/15\n",
      "102/102 [==============================] - 24s 231ms/step - loss: 0.1926 - accuracy: 0.9448\n",
      "Epoch 8/15\n",
      "102/102 [==============================] - 25s 246ms/step - loss: 0.1707 - accuracy: 0.9515\n",
      "Epoch 9/15\n",
      "102/102 [==============================] - 25s 241ms/step - loss: 0.1591 - accuracy: 0.9572\n",
      "Epoch 10/15\n",
      "102/102 [==============================] - 25s 247ms/step - loss: 0.1413 - accuracy: 0.9610\n",
      "Epoch 11/15\n",
      "102/102 [==============================] - 25s 243ms/step - loss: 0.1311 - accuracy: 0.9651\n",
      "Epoch 12/15\n",
      "102/102 [==============================] - 27s 263ms/step - loss: 0.1214 - accuracy: 0.9665\n",
      "Epoch 13/15\n",
      "102/102 [==============================] - 23s 227ms/step - loss: 0.1117 - accuracy: 0.9695\n",
      "Epoch 14/15\n",
      "102/102 [==============================] - 26s 253ms/step - loss: 0.1064 - accuracy: 0.9719\n",
      "Epoch 15/15\n",
      "102/102 [==============================] - 23s 230ms/step - loss: 0.1062 - accuracy: 0.9713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x193bfd764c0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# also adding weights\n",
    "class_weights = {0: 1 ,\n",
    "                1: 1.6/bias }\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "          class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "_uuid": "2803c5699e0aec22463aadccd1255e63155c1b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1453  172]\n",
      " [ 130  879]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91      1625\n",
      "           1       0.84      0.87      0.85      1009\n",
      "\n",
      "    accuracy                           0.89      2634\n",
      "   macro avg       0.88      0.88      0.88      2634\n",
      "weighted avg       0.89      0.89      0.89      2634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_x = model.predict(X_test) \n",
    "Y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c23e4d6b4cab5b46a58b4c2962abd7a0d29df42"
   },
   "source": [
    "So the class imbalance is reduced significantly recall value for positive tweets (Class 1) improved from 0.54 to 0.77. It is alwayes not possible to reduce it compleatly. \n",
    "\n",
    "You may also noticed that the recall value for Negative tweets also decreased from 0.90 to 0.78  but this can be improved using training model to more epocs and tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "_uuid": "085294a5409b58c2188019177041ceb1756f1826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1446  179]\n",
      " [ 122  887]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91      1625\n",
      "           1       0.83      0.88      0.85      1009\n",
      "\n",
      "    accuracy                           0.89      2634\n",
      "   macro avg       0.88      0.88      0.88      2634\n",
      "weighted avg       0.89      0.89      0.89      2634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running model to few more epochs\n",
    "# model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "#           class_weight=class_weights)\n",
    "\n",
    "predict_x = model.predict(X_test) \n",
    "Y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "# Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
    "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  66 498 109 363  74 349 230   3]]\n",
      "1/1 - 0s - 25ms/epoch - 25ms/step\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "twt = ['USER USER Buat film muslim pilih ahok mati nggak di sholatin...... dijamin laris']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=30, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"negative\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"positive\")"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
