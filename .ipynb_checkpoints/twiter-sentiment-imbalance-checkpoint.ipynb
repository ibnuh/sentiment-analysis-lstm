{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4",
    "_uuid": "b34dc51c4c60fc1cc8200129e74e7a025fd0cc42"
   },
   "source": [
    "**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n",
    "\n",
    "**What's New** I have added how to deal with data imbalance. Almost all classification task have this problem as number of data of every class if different. For current dataset number of data having positive sentiments is very low relative to data with negative sentiment.\n",
    "\n",
    "**Solving class imbalaned data**:\n",
    "- upsampling \n",
    "- using class weighted loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39c44f0e-d62c-7e11-a542-4fcd58e21442",
    "_uuid": "4efef6a6c3143fbb6bca5903fc1a764bbbb861c4"
   },
   "source": [
    "Using LSTM to classify the movie reviews into positive and negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
    "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
    "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5"
   },
   "source": [
    "Only keeping the necessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
    "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./clean_dataset.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['Tweet','HS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68989f3bc936825f4425d2d08467ce17c4a2f092"
   },
   "source": [
    "Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f6a102c71c8e281450f7e73a5678cc9d0bb99e99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>di saat cowok usaha lacak perhati gue kamu lan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telat beri tau kamu edan sarap gue gaul cigax ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kadang pikir percaya tuhan jatuh kali kali kad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tau mata sipit lihat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kafir sudah lihat dongok dungu haha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  di saat cowok usaha lacak perhati gue kamu lan...   1\n",
       "1  telat beri tau kamu edan sarap gue gaul cigax ...   0\n",
       "2  kadang pikir percaya tuhan jatuh kali kali kad...   0\n",
       "3                               tau mata sipit lihat   0\n",
       "4    kaum cebong kafir sudah lihat dongok dungu haha   1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
    "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20"
   },
   "source": [
    "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>di saat cowok usaha lacak perhati gue kamu lan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telat beri tau kamu edan sarap gue gaul cigax ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kadang pikir percaya tuhan jatuh kali kali kad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tau mata sipit lihat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kafir sudah lihat dongok dungu haha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bani taplak dan kawan kawan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deklarasi pilih kepala daerah aman anti hoaks ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gue saja selesai watch aldnoah zero kampret me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>admin belanja po baik nak makan ais kepal milo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enak kalau sambil ngewe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS\n",
       "0  di saat cowok usaha lacak perhati gue kamu lan...   1\n",
       "1  telat beri tau kamu edan sarap gue gaul cigax ...   0\n",
       "2  kadang pikir percaya tuhan jatuh kali kali kad...   0\n",
       "3                               tau mata sipit lihat   0\n",
       "4    kaum cebong kafir sudah lihat dongok dungu haha   1\n",
       "5                        bani taplak dan kawan kawan   1\n",
       "6  deklarasi pilih kepala daerah aman anti hoaks ...   0\n",
       "7  gue saja selesai watch aldnoah zero kampret me...   0\n",
       "8  admin belanja po baik nak makan ais kepal milo...   0\n",
       "9                            enak kalau sambil ngewe   0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# Set all text to be lowercase\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove special chars\n",
    "data['Tweet'] = data['Tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
    "\n",
    "# Define text to remove\n",
    "\n",
    "# data['Tweet'] = data['Tweet'].apply(lambda x: bytes(x, 'utf-8').decode('utf-8', 'ignore'))\n",
    "data['Tweet'] = data['Tweet'].apply((lambda x: ''.join([c for c in x if c not in punctuation])))\n",
    "data['Tweet'] = data['Tweet'].str.replace('rt', '')\n",
    "data['Tweet'] = data['Tweet'].str.replace('user', '')\n",
    "data['Tweet'] = data['Tweet'].str.replace('\\n', '')\n",
    "data['Tweet'] = data['Tweet'].str.strip()\n",
    "\n",
    "data.head(10)\n",
    "# print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13116 Total\n",
      "11106 Hate speech\n",
      "15126 Non hate speech\n"
     ]
    }
   ],
   "source": [
    "print(data['HS'].size, \"Total\")\n",
    "print(data[data['HS'] == 1].size, \"Hate speech\")\n",
    "print(data[data['HS'] == 0].size, \"Non hate speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,   57,  336,  165,  598,   10,   23,  598,\n",
       "          10,   81,  139,   23,   23,  336,  211],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0, 1890,  690,\n",
       "          41,   23,  452,  386,   10, 1686,   58],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  599,\n",
       "          94,  149,  198,  506,  129,  129,  599,  198,  216,    5,  482,\n",
       "         982, 1687, 1011,   13,   32,   34, 1786]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values)\n",
    "X = tokenizer.texts_to_sequences(data['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
    "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89"
   },
   "source": [
    "Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 40, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 40, 128)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 196)               254800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
    "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162"
   },
   "source": [
    "Hereby I declare the train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
    "_uuid": "a380bbfae2d098d407b138fc44622c9913a31c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10492, 40) (10492, 2)\n",
      "(2624, 40) (2624, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['HS']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
    "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7"
   },
   "source": [
    "Here we train the Network with 15 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "82/82 [==============================] - 18s 199ms/step - loss: 0.5415 - accuracy: 0.7060\n",
      "Epoch 2/15\n",
      "82/82 [==============================] - 20s 241ms/step - loss: 0.3541 - accuracy: 0.8447\n",
      "Epoch 3/15\n",
      "82/82 [==============================] - 22s 269ms/step - loss: 0.3184 - accuracy: 0.8643\n",
      "Epoch 4/15\n",
      "82/82 [==============================] - 21s 258ms/step - loss: 0.2924 - accuracy: 0.8765\n",
      "Epoch 5/15\n",
      "82/82 [==============================] - 23s 275ms/step - loss: 0.2686 - accuracy: 0.8841\n",
      "Epoch 6/15\n",
      "82/82 [==============================] - 23s 277ms/step - loss: 0.2476 - accuracy: 0.8977\n",
      "Epoch 7/15\n",
      "82/82 [==============================] - 25s 300ms/step - loss: 0.2355 - accuracy: 0.9016\n",
      "Epoch 8/15\n",
      "82/82 [==============================] - 23s 275ms/step - loss: 0.2159 - accuracy: 0.9116\n",
      "Epoch 9/15\n",
      "82/82 [==============================] - 27s 327ms/step - loss: 0.2013 - accuracy: 0.9168\n",
      "Epoch 10/15\n",
      "82/82 [==============================] - 25s 305ms/step - loss: 0.1915 - accuracy: 0.9190\n",
      "Epoch 11/15\n",
      "82/82 [==============================] - 30s 369ms/step - loss: 0.1759 - accuracy: 0.9301\n",
      "Epoch 12/15\n",
      "82/82 [==============================] - 32s 384ms/step - loss: 0.1612 - accuracy: 0.9343\n",
      "Epoch 13/15\n",
      "82/82 [==============================] - 27s 325ms/step - loss: 0.1537 - accuracy: 0.9379\n",
      "Epoch 14/15\n",
      "82/82 [==============================] - 26s 313ms/step - loss: 0.1477 - accuracy: 0.9387\n",
      "Epoch 15/15\n",
      "82/82 [==============================] - 26s 313ms/step - loss: 0.1372 - accuracy: 0.9419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d71981a90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434",
    "_uuid": "47e99d7ed1f27a85eb01dbafc71b66b329fb1d12"
   },
   "source": [
    "Extracting a validation set, and measuring score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "4701961e44bd243e505fc2c1b53b323311ad2b80"
   },
   "outputs": [],
   "source": [
    "# Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "predict_x = model.predict(X_test) \n",
    "classes_x = np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "13d9f618b3b74f881d68aed864a26249d26e63de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1286  221]\n",
      " [ 276  841]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84      1507\n",
      "           1       0.79      0.75      0.77      1117\n",
      "\n",
      "    accuracy                           0.81      2624\n",
      "   macro avg       0.81      0.80      0.80      2624\n",
      "weighted avg       0.81      0.81      0.81      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':classes_x})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf",
    "_uuid": "4b54f18bbf22a953c60f271c318cb076e684df9c"
   },
   "source": [
    "Finally measuring the number of correct guesses.  It is clear that finding negative tweets (**class 0**) goes very well (**recall 0.92**) for the Network but deciding whether is positive (**class 1**) is not really (**recall 0.52**). My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "890a03c9-316e-4d55-98e1-ba29045eff6c",
    "_uuid": "cfcbefe939b72297019e221ca3f5a283974bffff"
   },
   "source": [
    "As expected accuracy for positive data is vary low compare to negative, Lets try to solve this problem.\n",
    "\n",
    "**1. Up-sample Minority Class**\n",
    "\n",
    "Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal.\n",
    "There are several heuristics for doing so, but the most common way is to simply resample with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52"
   },
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "data_majority = data[data['HS'] == 0]\n",
    "data_minority = data[data['HS'] == 1]\n",
    "\n",
    "bias = data_minority.shape[0]/data_majority.shape[0]\n",
    "# lets split train/test data first then \n",
    "train = pd.concat([data_majority.sample(frac=0.8,random_state=200),\n",
    "         data_minority.sample(frac=0.8,random_state=200)])\n",
    "test = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),\n",
    "        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "ce10e582bd894ec271f2587ceb618a9cf8ab03c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive data in training: 4442\n",
      "negative data in training: 6050\n",
      "positive data in test: 1111\n",
      "negative data in test: 1513\n"
     ]
    }
   ],
   "source": [
    "print('positive data in training:',(train.HS == 1).sum())\n",
    "print('negative data in training:',(train.HS == 0).sum())\n",
    "print('positive data in test:',(test.HS == 1).sum())\n",
    "print('negative data in test:',(test.HS == 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority class before upsample: (6050, 2)\n",
      "minority class before upsample: (4442, 2)\n",
      "After upsampling\n",
      "0    6050\n",
      "1    6050\n",
      "Name: HS, dtype: int64\n",
      "x_train shape: (12100, 29)\n",
      "x_test shape (2624, 29)\n"
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes in training data for upsampling \n",
    "data_majority = train[train['HS'] == 0]\n",
    "data_minority = train[train['HS'] == 1]\n",
    "\n",
    "print(\"majority class before upsample:\",data_majority.shape)\n",
    "print(\"minority class before upsample:\",data_minority.shape)\n",
    "\n",
    "# Upsample minority class\n",
    "data_minority_upsampled = resample(data_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "print(\"After upsampling\\n\",data_upsampled.HS.value_counts(),sep = \"\")\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values) # training with whole data\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(data_upsampled['Tweet'].values)\n",
    "X_train = pad_sequences(X_train,maxlen=29)\n",
    "Y_train = pd.get_dummies(data_upsampled['HS']).values\n",
    "print('x_train shape:',X_train.shape)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test['Tweet'].values)\n",
    "X_test = pad_sequences(X_test,maxlen=29)\n",
    "Y_test = pd.get_dummies(test['HS']).values\n",
    "print(\"x_test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 29, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 29, 128)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 192)               246528    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 386       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 502,914\n",
      "Trainable params: 502,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "embed_dim = 128\n",
    "lstm_out = 192\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
    "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7"
   },
   "source": [
    "Here we train the Network. We should run much more than 15 epoch, but I would have to wait forever for kaggle, so it is 15 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "95/95 [==============================] - 16s 141ms/step - loss: 0.7943 - accuracy: 0.6384\n",
      "Epoch 2/15\n",
      "95/95 [==============================] - 14s 151ms/step - loss: 0.4842 - accuracy: 0.8386\n",
      "Epoch 3/15\n",
      "95/95 [==============================] - 18s 194ms/step - loss: 0.4274 - accuracy: 0.8621\n",
      "Epoch 4/15\n",
      "95/95 [==============================] - 17s 175ms/step - loss: 0.3851 - accuracy: 0.8752\n",
      "Epoch 5/15\n",
      "95/95 [==============================] - 17s 176ms/step - loss: 0.3618 - accuracy: 0.8846\n",
      "Epoch 6/15\n",
      "95/95 [==============================] - 19s 200ms/step - loss: 0.3374 - accuracy: 0.8931\n",
      "Epoch 7/15\n",
      "95/95 [==============================] - 17s 178ms/step - loss: 0.3074 - accuracy: 0.9048\n",
      "Epoch 8/15\n",
      "95/95 [==============================] - 17s 174ms/step - loss: 0.2878 - accuracy: 0.9127\n",
      "Epoch 9/15\n",
      "95/95 [==============================] - 20s 213ms/step - loss: 0.2719 - accuracy: 0.9182\n",
      "Epoch 10/15\n",
      "95/95 [==============================] - 18s 190ms/step - loss: 0.2591 - accuracy: 0.9203\n",
      "Epoch 11/15\n",
      "95/95 [==============================] - 18s 192ms/step - loss: 0.2450 - accuracy: 0.9262\n",
      "Epoch 12/15\n",
      "95/95 [==============================] - 19s 201ms/step - loss: 0.2294 - accuracy: 0.9318\n",
      "Epoch 13/15\n",
      "95/95 [==============================] - 17s 182ms/step - loss: 0.2183 - accuracy: 0.9345\n",
      "Epoch 14/15\n",
      "95/95 [==============================] - 18s 185ms/step - loss: 0.2146 - accuracy: 0.9350\n",
      "Epoch 15/15\n",
      "95/95 [==============================] - 19s 203ms/step - loss: 0.1965 - accuracy: 0.9402s - loss: 0.195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d7e6826d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# also adding weights\n",
    "class_weights = {0: 1 ,\n",
    "                1: 1.6/bias }\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "          class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "2803c5699e0aec22463aadccd1255e63155c1b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1212  301]\n",
      " [ 236  875]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      1513\n",
      "           1       0.74      0.79      0.77      1111\n",
      "\n",
      "    accuracy                           0.80      2624\n",
      "   macro avg       0.79      0.79      0.79      2624\n",
      "weighted avg       0.80      0.80      0.80      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_x = model.predict(X_test) \n",
    "Y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c23e4d6b4cab5b46a58b4c2962abd7a0d29df42"
   },
   "source": [
    "So the class imbalance is reduced significantly recall value for positive tweets (Class 1) improved from 0.54 to 0.77. It is alwayes not possible to reduce it compleatly. \n",
    "\n",
    "You may also noticed that the recall value for Negative tweets also decreased from 0.90 to 0.78  but this can be improved using training model to more epocs and tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "085294a5409b58c2188019177041ceb1756f1826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[1212  301]\n",
      " [ 236  875]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      1513\n",
      "           1       0.74      0.79      0.77      1111\n",
      "\n",
      "    accuracy                           0.80      2624\n",
      "   macro avg       0.79      0.79      0.79      2624\n",
      "weighted avg       0.80      0.80      0.80      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running model to few more epochs\n",
    "# model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "#           class_weight=class_weights)\n",
    "\n",
    "predict_x = model.predict(X_test) \n",
    "Y_pred = np.argmax(predict_x,axis=1)\n",
    "\n",
    "# Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
    "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0  217 1134   96  217   25  212\n",
      "   723]]\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "[0.002676 0.997324]\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "twt = ['ayo kita makan bersama, ayo cebong ikut dong']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=29, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "print(sentiment)\n",
    "\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"negative\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"positive\")"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
